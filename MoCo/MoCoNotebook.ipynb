{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115582,"status":"ok","timestamp":1718481375414,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"},"user_tz":-60},"id":"UzEKOl2iy3fJ","outputId":"900184bb-a6cb-46ac-e58c-4a616bf91fdf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n","Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install torch torchvision torchmetrics\n","\n","import argparse\n","import builtins\n","import math\n","import os\n","import random\n","import shutil\n","import time\n","import warnings\n","\n","import torch\n","import torch.backends.cudnn as cudnn\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","import torch.utils.data.distributed\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import torchmetrics\n","import torch.optim\n","\n","import numpy as np\n","from torch.utils.data import Dataset\n","\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":71,"status":"ok","timestamp":1718481375415,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"},"user_tz":-60},"id":"YpvBnlLii40_"},"outputs":[],"source":["class ImageEncoder(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.net = models.resnet50(weights=None)\n","        self.net.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.net.maxpool = nn.Identity()\n","        #self.net.fc = nn.Identity()  # You might still declare it to avoid errors, but it won't be used.\n","        del self.net.fc\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # Manually handle each step, avoiding the original 'fc' layer\n","        x = self.net.conv1(x)\n","        x = self.net.bn1(x)\n","        x = self.net.relu(x)\n","        x = self.net.maxpool(x)  # Identity, so no effect\n","        x = self.net.layer1(x)\n","        x = self.net.layer2(x)\n","        x = self.net.layer3(x)\n","        x = self.net.layer4(x)\n","        x = self.net.avgpool(x)  # AdaptiveAvgPool2d at the end of ResNet\n","        x = torch.flatten(x, 1)\n","        # Do not call self.net.fc(x) as 'fc' is removed\n","        return x\n","\n","class MoCo(nn.Module):\n","    \"\"\"\n","    Build a MoCo model with: a query encoder, a key encoder, and a queue\n","    https://arxiv.org/abs/1911.05722\n","    \"\"\"\n","\n","    def __init__(self, base_encoder, dim=128, K=65536, m=0.999, T=0.07, size=224, mlp=False):\n","        \"\"\"\n","        dim: feature dimension (default: 128)\n","        K: queue size; number of negative keys (default: 65536)\n","        m: moco momentum of updating key encoder (default: 0.999)\n","        T: softmax temperature (default: 0.07)\n","        \"\"\"\n","        super(MoCo, self).__init__()\n","\n","        self.K = K\n","        self.m = m\n","        self.T = T\n","        self.size = size\n","\n","        # create the encoders\n","        # num_classes is the output fc dimension\n","        self.encoder_q = base_encoder()\n","        self.encoder_k = base_encoder()\n","\n","        if mlp:  # hack: brute-force replacement\n","            dim_mlp = self.encoder_q(torch.zeros(1, 1, self.size, self.size)).shape[1]\n","            self.encoder_q = nn.Sequential(\n","                self.encoder_q,\n","                nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, dim)\n","            )\n","            self.encoder_k = nn.Sequential(\n","                self.encoder_k,\n","                nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, dim)\n","            )\n","\n","        for param_q, param_k in zip(\n","            self.encoder_q.parameters(), self.encoder_k.parameters()\n","        ):\n","            param_k.data.copy_(param_q.data)  # initialize\n","            param_k.requires_grad = False  # not update by gradient\n","\n","        # create the queue\n","        self.register_buffer(\"queue\", torch.randn(dim, K))\n","        self.queue = torch.randn(dim, K)\n","        self.queue = nn.functional.normalize(self.queue, dim=0)\n","\n","        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n","\n","    @torch.no_grad()\n","    def _momentum_update_key_encoder(self):\n","        \"\"\"\n","        Momentum update of the key encoder\n","        \"\"\"\n","        for param_q, param_k in zip(\n","            self.encoder_q.parameters(), self.encoder_k.parameters()\n","        ):\n","            param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)\n","\n","    @torch.no_grad()\n","    def _dequeue_and_enqueue(self, keys):\n","        # gather keys before updating queue\n","        #keys = concat_all_gather(keys)\n","\n","        batch_size = keys.shape[0]\n","\n","        ptr = int(self.queue_ptr)\n","        assert self.K % batch_size == 0  # for simplicity\n","\n","        # replace the keys at ptr (dequeue and enqueue)\n","        self.queue[:, ptr : ptr + batch_size] = keys.T\n","        ptr = (ptr + batch_size) % self.K  # move pointer\n","\n","        self.queue_ptr[0] = ptr\n","\n","    @torch.no_grad()\n","    def _batch_shuffle_ddp(self, x):\n","        \"\"\"\n","        Batch shuffle, for making use of BatchNorm.\n","        *** Only support DistributedDataParallel (DDP) model. ***\n","        \"\"\"\n","        # gather from all gpus\n","        batch_size_this = x.shape[0]\n","        x_gather = concat_all_gather(x)\n","        batch_size_all = x_gather.shape[0]\n","\n","        num_gpus = batch_size_all // batch_size_this\n","\n","        # random shuffle index\n","        idx_shuffle = torch.randperm(batch_size_all).to(device)\n","\n","        # broadcast to all gpus\n","        torch.distributed.broadcast(idx_shuffle, src=0)\n","\n","        # index for restoring\n","        idx_unshuffle = torch.argsort(idx_shuffle)\n","\n","        # shuffled index for this gpu\n","        gpu_idx = torch.distributed.get_rank()\n","        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]\n","\n","        return x_gather[idx_this], idx_unshuffle\n","\n","    @torch.no_grad()\n","    def _batch_unshuffle_ddp(self, x, idx_unshuffle):\n","        \"\"\"\n","        Undo batch shuffle.\n","        *** Only support DistributedDataParallel (DDP) model. ***\n","        \"\"\"\n","        # gather from all gpus\n","        batch_size_this = x.shape[0]\n","        x_gather = concat_all_gather(x)\n","        batch_size_all = x_gather.shape[0]\n","\n","        num_gpus = batch_size_all // batch_size_this\n","\n","        # restored index for this gpu\n","        gpu_idx = torch.distributed.get_rank()\n","        idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]\n","\n","        return x_gather[idx_this]\n","\n","    def forward(self, im_q, im_k):\n","        \"\"\"\n","        Input:\n","            im_q: a batch of query images\n","            im_k: a batch of key images\n","        Output:\n","            logits, targets\n","        \"\"\"\n","\n","        # compute query features\n","        q = self.encoder_q(im_q)  # queries: NxC\n","        q = nn.functional.normalize(q, dim=1)\n","\n","        # compute key features\n","        with torch.no_grad():  # no gradient to keys\n","            self._momentum_update_key_encoder()  # update the key encoder\n","\n","            k = self.encoder_k(im_k)  # keys: NxC\n","            k = nn.functional.normalize(k, dim=1)\n","\n","        # compute logits\n","        # Einstein sum is more intuitive\n","        # positive logits: Nx1\n","        l_pos = torch.einsum(\"nc,nc->n\", [q, k]).unsqueeze(-1)\n","        # negative logits: NxK\n","        l_neg = torch.einsum(\"nc,ck->nk\", [q, self.queue.clone().detach()])\n","\n","        # logits: Nx(1+K)\n","        logits = torch.cat([l_pos, l_neg], dim=1)\n","\n","        # apply temperature\n","        logits /= self.T\n","\n","        # labels: positive key indicators\n","        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(logits.device)\n","\n","        # dequeue and enqueue\n","        self._dequeue_and_enqueue(k)\n","\n","        return logits, labels\n","\n","\n","# utils\n","@torch.no_grad()\n","def concat_all_gather(tensor):\n","    \"\"\"\n","    Performs all_gather operation on the provided tensors.\n","    *** Warning ***: torch.distributed.all_gather has no gradient.\n","    \"\"\"\n","    tensors_gather = [\n","        torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())\n","    ]\n","    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n","\n","    output = torch.cat(tensors_gather, dim=0)\n","    return output"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":69,"status":"ok","timestamp":1718481375415,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"},"user_tz":-60},"id":"6MGdxHd3ymGP"},"outputs":[],"source":["import random\n","\n","from PIL import ImageFilter\n","\n","\n","class TwoCropsTransform:\n","    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n","\n","    def __init__(self, base_transform):\n","        self.base_transform = base_transform\n","\n","    def __call__(self, x):\n","        q = self.base_transform(x)\n","        k = self.base_transform(x)\n","        return [q, k]\n","\n","\n","class GaussianBlur:\n","    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n","\n","    def __init__(self, sigma=[0.1, 2.0]):\n","        self.sigma = sigma\n","\n","    def __call__(self, x):\n","        sigma = random.uniform(self.sigma[0], self.sigma[1])\n","        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n","        return x"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":69,"status":"ok","timestamp":1718481375415,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"},"user_tz":-60},"id":"zJuZ5KwIcGsZ"},"outputs":[],"source":["def main():\n","    # Define arguments directly in the code for simplicity\n","    class Args:\n","        data = \"/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/MoCo\"  # Update this as needed\n","        arch = \"resnet50\"\n","        workers = 1\n","        epochs = 200 #base:200\n","        start_epoch = 0\n","        size = 24 #size of the image in NxN pixels\n","        batch_size = 128  # Lowering for Colab\n","        lr = 0.03\n","        schedule = [120, 160]\n","        momentum = 0.9\n","        weight_decay = 1e-4\n","        print_freq = 10\n","        resume = \"\"  # If needed, specify checkpoint path\n","        seed = None\n","        gpu = 0\n","        distributed = False\n","\n","        # MoCo-specific configs\n","        moco_dim = 2048\n","        moco_k = 65536\n","        moco_m = 0.99 #def = 0.999\n","        moco_t = 0.07\n","        mlp = False\n","        aug_plus = True\n","        cos = False\n","\n","    args = Args()\n","\n","    if args.seed is not None:\n","        random.seed(args.seed)\n","        torch.manual_seed(args.seed)\n","        cudnn.deterministic = True\n","\n","    # Check for GPU availability\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Create the model\n","    print(\"=> creating model '{}'\".format(args.arch))\n","\n","    model = MoCo(\n","        ImageEncoder,\n","        args.moco_dim,\n","        args.moco_k,\n","        args.moco_m,\n","        args.moco_t,\n","        args.size,\n","        args.mlp,\n","    )\n","    model = model.to(device)\n","    print(model)\n","\n","    # Define loss function and optimizer\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    optimizer = torch.optim.SGD(\n","        model.parameters(),\n","        args.lr,\n","        momentum=args.momentum,\n","        weight_decay=args.weight_decay,\n","    )\n","\n","    # Optionally resume from a checkpoint\n","    if args.resume:\n","        if os.path.isfile(args.resume):\n","            print(\"=> loading checkpoint '{}'\".format(args.resume))\n","            checkpoint = torch.load(args.resume, map_location=device)\n","            args.start_epoch = checkpoint[\"epoch\"]\n","            model.load_state_dict(checkpoint[\"state_dict\"])\n","            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","            print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint[\"epoch\"]))\n","        else:\n","            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n","\n","    cudnn.benchmark = True\n","\n","    # Data loading code\n","    traindir = os.path.join(args.data, \"train\")\n","    normalize = transforms.Normalize(\n","        mean=[0.485], std=[0.229]\n","    )\n","\n","    if args.aug_plus:\n","        # MoCo v2's aug adjusted to greyscale\n","        augmentation = [\n","            transforms.ToPILImage(),\n","            transforms.RandomResizedCrop(args.size, scale=(0.2, 1.0)),\n","            transforms.RandomApply([transforms.ColorJitter(0.2,0.2)], p=0.8),\n","            transforms.RandomApply([GaussianBlur([0.1, 2.0])], p=0.5),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            normalize,\n","        ]\n","    else:\n","        # MoCo v1's aug adjusted to greyscale\n","        augmentation = [\n","            transforms.ToPILImage(),\n","            transforms.RandomResizedCrop(args.size, scale=(0.2, 1.0)),\n","            transforms.ColorJitter(0.2, 0.2),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            normalize,\n","        ]\n","\n","    two_crops_transform = TwoCropsTransform(transforms.Compose(augmentation))\n","\n","    train_dataset = NPZDataset('/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/SimCLR/ECG5000_processed.npz', mode='train', transform=two_crops_transform)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=args.batch_size,\n","        shuffle=True,\n","        num_workers=args.workers,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","\n","    for epoch in range(args.start_epoch, args.epochs):\n","        adjust_learning_rate(optimizer, epoch, args)\n","        train(train_loader, model, criterion, optimizer, epoch, args, device)\n","\n","        # Save checkpoint every 10 epochs\n","        if epoch % 199 == 0:\n","            save_checkpoint(\n","                {\n","                    \"epoch\": epoch + 1,\n","                    \"arch\": args.arch,\n","                    \"state_dict\": model.state_dict(),\n","                    \"optimizer\": optimizer.state_dict(),\n","                },\n","                filename=\"/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/MoCo/checkpoint_{:04d}.pth.tar\".format(epoch),\n","            )\n","\n","def train(train_loader, model, criterion, optimizer, epoch, args, device):\n","    batch_time = AverageMeter(\"Time\", \":6.3f\")\n","    data_time = AverageMeter(\"Data\", \":6.3f\")\n","    losses = AverageMeter(\"Loss\", \":.4e\")\n","    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n","    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n","    progress = ProgressMeter(\n","        len(train_loader),\n","        [batch_time, data_time, losses, top1, top5],\n","        prefix=\"Epoch: [{}]\".format(epoch),\n","    )\n","\n","    # Switch to train mode\n","    model.train()\n","\n","    end = time.time()\n","\n","    for i, (images, _) in enumerate(train_loader):\n","        data_time.update(time.time() - end)\n","\n","        images[0] = images[0].to(device, non_blocking=True)\n","        images[1] = images[1].to(device, non_blocking=True)\n","\n","        output, target = model(im_q=images[0], im_k=images[1])\n","        loss = criterion(output, target)\n","\n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        losses.update(loss.item(), images[0].size(0))\n","        top1.update(acc1[0], images[0].size(0))\n","        top5.update(acc5[0], images[0].size(0))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % args.print_freq == 0:\n","            progress.display(i)\n","\n","def save_checkpoint(state, filename=\"/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/MoCo/checkpoint.pth.tar\"):\n","    torch.save(state, filename)\n","\n","class AverageMeter:\n","    def __init__(self, name, fmt=\":f\"):\n","        self.name = name\n","        self.fmt = fmt\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","    def __str__(self):\n","        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n","        return fmtstr.format(**self.__dict__)\n","\n","class ProgressMeter:\n","    def __init__(self, num_batches, meters, prefix=\"\"):\n","        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n","        self.meters = meters\n","        self.prefix = prefix\n","\n","    def display(self, batch):\n","        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n","        entries += [str(meter) for meter in self.meters]\n","        print(\"\\t\".join(entries))\n","\n","    def _get_batch_fmtstr(self, num_batches):\n","        num_digits = len(str(num_batches))\n","        fmt = \"{:\" + str(num_digits) + \"d}\"\n","        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n","\n","def adjust_learning_rate(optimizer, epoch, args):\n","    lr = args.lr\n","    if args.cos:\n","        lr *= 0.5 * (1.0 + math.cos(math.pi * epoch / args.epochs))\n","    else:\n","        for milestone in args.schedule:\n","            lr *= 0.1 if epoch >= milestone else 1.0\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","def accuracy(output, target, topk=(1,)):\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        _, pred = output.topk(maxk, 1, True, True)\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True) #view -> reshape\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":68,"status":"ok","timestamp":1718481375415,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"},"user_tz":-60},"id":"QqbO9_79QBMa"},"outputs":[],"source":["class NPZDataset(Dataset):\n","    def __init__(self, file_path, mode='train', transform=None):\n","        data = np.load(file_path)\n","        self.images = data[f'{mode}_images']\n","        self.labels = data[f'{mode}_labels']  # Note: Labels might not be used in MoCo training\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        if self.transform:\n","          q, k = self.transform(image)\n","        return [q, k], idx  # MoCo typically doesn't use labels directly in the loss calculation"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sLqxBpgGcn6U","executionInfo":{"status":"ok","timestamp":1718481756668,"user_tz":-60,"elapsed":381321,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"}},"outputId":"71311d3f-71cc-4ec9-9c97-bc5b21ba340c"},"outputs":[{"output_type":"stream","name":"stdout","text":["=> creating model 'resnet50'\n","MoCo(\n","  (encoder_q): ImageEncoder(\n","    (net): ResNet(\n","      (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): Identity()\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    )\n","  )\n","  (encoder_k): ImageEncoder(\n","    (net): ResNet(\n","      (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): Identity()\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    )\n","  )\n",")\n","Epoch: [0][0/3]\tTime 11.350 (11.350)\tData  0.466 ( 0.466)\tLoss 3.3179e-01 (3.3179e-01)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [1][0/3]\tTime  1.244 ( 1.244)\tData  0.342 ( 0.342)\tLoss 5.9734e+00 (5.9734e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   3.91 (  3.91)\n","Epoch: [2][0/3]\tTime  0.704 ( 0.704)\tData  0.268 ( 0.268)\tLoss 6.5918e+00 (6.5918e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   1.56 (  1.56)\n","Epoch: [3][0/3]\tTime  0.664 ( 0.664)\tData  0.240 ( 0.240)\tLoss 7.0403e+00 (7.0403e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\n","Epoch: [4][0/3]\tTime  0.644 ( 0.644)\tData  0.225 ( 0.225)\tLoss 7.2819e+00 (7.2819e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   0.78 (  0.78)\n","Epoch: [5][0/3]\tTime  0.791 ( 0.791)\tData  0.285 ( 0.285)\tLoss 7.5595e+00 (7.5595e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   0.78 (  0.78)\n","Epoch: [6][0/3]\tTime  0.667 ( 0.667)\tData  0.242 ( 0.242)\tLoss 7.6451e+00 (7.6451e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\n","Epoch: [7][0/3]\tTime  0.715 ( 0.715)\tData  0.240 ( 0.240)\tLoss 7.8354e+00 (7.8354e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.78 (  0.78)\n","Epoch: [8][0/3]\tTime  0.732 ( 0.732)\tData  0.264 ( 0.264)\tLoss 7.8777e+00 (7.8777e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.78 (  0.78)\n","Epoch: [9][0/3]\tTime  1.088 ( 1.088)\tData  0.350 ( 0.350)\tLoss 8.0487e+00 (8.0487e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\n","Epoch: [10][0/3]\tTime  1.154 ( 1.154)\tData  0.375 ( 0.375)\tLoss 8.1038e+00 (8.1038e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.78 (  0.78)\n","Epoch: [11][0/3]\tTime  0.693 ( 0.693)\tData  0.248 ( 0.248)\tLoss 8.2104e+00 (8.2104e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.78 (  0.78)\n","Epoch: [12][0/3]\tTime  0.716 ( 0.716)\tData  0.256 ( 0.256)\tLoss 8.2660e+00 (8.2660e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   2.34 (  2.34)\n","Epoch: [13][0/3]\tTime  0.719 ( 0.719)\tData  0.262 ( 0.262)\tLoss 8.3920e+00 (8.3920e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   1.56 (  1.56)\n","Epoch: [14][0/3]\tTime  0.725 ( 0.725)\tData  0.279 ( 0.279)\tLoss 8.4388e+00 (8.4388e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   3.12 (  3.12)\n","Epoch: [15][0/3]\tTime  0.697 ( 0.697)\tData  0.246 ( 0.246)\tLoss 8.4586e+00 (8.4586e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   2.34 (  2.34)\n","Epoch: [16][0/3]\tTime  0.715 ( 0.715)\tData  0.270 ( 0.270)\tLoss 8.4791e+00 (8.4791e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   3.12 (  3.12)\n","Epoch: [17][0/3]\tTime  0.829 ( 0.829)\tData  0.260 ( 0.260)\tLoss 8.5545e+00 (8.5545e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   3.12 (  3.12)\n","Epoch: [18][0/3]\tTime  1.162 ( 1.162)\tData  0.431 ( 0.431)\tLoss 8.6484e+00 (8.6484e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   1.56 (  1.56)\n","Epoch: [19][0/3]\tTime  1.061 ( 1.061)\tData  0.361 ( 0.361)\tLoss 8.5592e+00 (8.5592e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   3.91 (  3.91)\n","Epoch: [20][0/3]\tTime  0.675 ( 0.675)\tData  0.228 ( 0.228)\tLoss 8.6513e+00 (8.6513e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   3.12 (  3.12)\n","Epoch: [21][0/3]\tTime  0.666 ( 0.666)\tData  0.233 ( 0.233)\tLoss 8.6927e+00 (8.6927e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  10.16 ( 10.16)\n","Epoch: [22][0/3]\tTime  0.658 ( 0.658)\tData  0.231 ( 0.231)\tLoss 8.5757e+00 (8.5757e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   4.69 (  4.69)\n","Epoch: [23][0/3]\tTime  0.673 ( 0.673)\tData  0.234 ( 0.234)\tLoss 8.5003e+00 (8.5003e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   9.38 (  9.38)\n","Epoch: [24][0/3]\tTime  0.671 ( 0.671)\tData  0.244 ( 0.244)\tLoss 8.6359e+00 (8.6359e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   7.81 (  7.81)\n","Epoch: [25][0/3]\tTime  0.659 ( 0.659)\tData  0.229 ( 0.229)\tLoss 8.6614e+00 (8.6614e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   4.69 (  4.69)\n","Epoch: [26][0/3]\tTime  1.014 ( 1.014)\tData  0.319 ( 0.319)\tLoss 8.7081e+00 (8.7081e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   2.34 (  2.34)\n","Epoch: [27][0/3]\tTime  1.118 ( 1.118)\tData  0.362 ( 0.362)\tLoss 8.8346e+00 (8.8346e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   6.25 (  6.25)\n","Epoch: [28][0/3]\tTime  0.660 ( 0.660)\tData  0.236 ( 0.236)\tLoss 8.7955e+00 (8.7955e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  10.16 ( 10.16)\n","Epoch: [29][0/3]\tTime  0.673 ( 0.673)\tData  0.248 ( 0.248)\tLoss 8.7428e+00 (8.7428e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   3.91 (  3.91)\n","Epoch: [30][0/3]\tTime  0.665 ( 0.665)\tData  0.236 ( 0.236)\tLoss 8.6174e+00 (8.6174e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   5.47 (  5.47)\n","Epoch: [31][0/3]\tTime  0.688 ( 0.688)\tData  0.233 ( 0.233)\tLoss 8.7790e+00 (8.7790e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   7.03 (  7.03)\n","Epoch: [32][0/3]\tTime  0.667 ( 0.667)\tData  0.231 ( 0.231)\tLoss 8.7763e+00 (8.7763e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   7.03 (  7.03)\n","Epoch: [33][0/3]\tTime  0.678 ( 0.678)\tData  0.241 ( 0.241)\tLoss 8.5289e+00 (8.5289e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   0.78 (  0.78)\n","Epoch: [34][0/3]\tTime  0.693 ( 0.693)\tData  0.244 ( 0.244)\tLoss 8.5377e+00 (8.5377e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   8.59 (  8.59)\n","Epoch: [35][0/3]\tTime  0.932 ( 0.932)\tData  0.344 ( 0.344)\tLoss 8.6574e+00 (8.6574e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   6.25 (  6.25)\n","Epoch: [36][0/3]\tTime  1.062 ( 1.062)\tData  0.380 ( 0.380)\tLoss 8.6563e+00 (8.6563e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   3.12 (  3.12)\n","Epoch: [37][0/3]\tTime  0.677 ( 0.677)\tData  0.245 ( 0.245)\tLoss 8.8258e+00 (8.8258e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   7.03 (  7.03)\n","Epoch: [38][0/3]\tTime  0.674 ( 0.674)\tData  0.246 ( 0.246)\tLoss 8.9087e+00 (8.9087e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   7.81 (  7.81)\n","Epoch: [39][0/3]\tTime  0.664 ( 0.664)\tData  0.234 ( 0.234)\tLoss 8.6907e+00 (8.6907e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   8.59 (  8.59)\n","Epoch: [40][0/3]\tTime  0.668 ( 0.668)\tData  0.231 ( 0.231)\tLoss 8.6327e+00 (8.6327e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  14.06 ( 14.06)\n","Epoch: [41][0/3]\tTime  0.718 ( 0.718)\tData  0.278 ( 0.278)\tLoss 8.6383e+00 (8.6383e+00)\tAcc@1   4.69 (  4.69)\tAcc@5   9.38 (  9.38)\n","Epoch: [42][0/3]\tTime  0.672 ( 0.672)\tData  0.242 ( 0.242)\tLoss 8.6391e+00 (8.6391e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  10.16 ( 10.16)\n","Epoch: [43][0/3]\tTime  0.845 ( 0.845)\tData  0.247 ( 0.247)\tLoss 8.9416e+00 (8.9416e+00)\tAcc@1   4.69 (  4.69)\tAcc@5   8.59 (  8.59)\n","Epoch: [44][0/3]\tTime  1.062 ( 1.062)\tData  0.352 ( 0.352)\tLoss 8.5948e+00 (8.5948e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   8.59 (  8.59)\n","Epoch: [45][0/3]\tTime  0.781 ( 0.781)\tData  0.340 ( 0.340)\tLoss 8.9416e+00 (8.9416e+00)\tAcc@1   4.69 (  4.69)\tAcc@5   7.81 (  7.81)\n","Epoch: [46][0/3]\tTime  0.678 ( 0.678)\tData  0.249 ( 0.249)\tLoss 8.8818e+00 (8.8818e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   9.38 (  9.38)\n","Epoch: [47][0/3]\tTime  0.664 ( 0.664)\tData  0.231 ( 0.231)\tLoss 8.4070e+00 (8.4070e+00)\tAcc@1   6.25 (  6.25)\tAcc@5  10.94 ( 10.94)\n","Epoch: [48][0/3]\tTime  0.691 ( 0.691)\tData  0.241 ( 0.241)\tLoss 8.7612e+00 (8.7612e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  10.94 ( 10.94)\n","Epoch: [49][0/3]\tTime  0.676 ( 0.676)\tData  0.241 ( 0.241)\tLoss 8.7757e+00 (8.7757e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  11.72 ( 11.72)\n","Epoch: [50][0/3]\tTime  0.705 ( 0.705)\tData  0.248 ( 0.248)\tLoss 9.0501e+00 (9.0501e+00)\tAcc@1   3.91 (  3.91)\tAcc@5   9.38 (  9.38)\n","Epoch: [51][0/3]\tTime  0.690 ( 0.690)\tData  0.240 ( 0.240)\tLoss 8.6818e+00 (8.6818e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   5.47 (  5.47)\n","Epoch: [52][0/3]\tTime  0.971 ( 0.971)\tData  0.360 ( 0.360)\tLoss 8.6565e+00 (8.6565e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   8.59 (  8.59)\n","Epoch: [53][0/3]\tTime  1.060 ( 1.060)\tData  0.327 ( 0.327)\tLoss 8.6895e+00 (8.6895e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  13.28 ( 13.28)\n","Epoch: [54][0/3]\tTime  0.676 ( 0.676)\tData  0.227 ( 0.227)\tLoss 8.8467e+00 (8.8467e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   8.59 (  8.59)\n","Epoch: [55][0/3]\tTime  0.680 ( 0.680)\tData  0.235 ( 0.235)\tLoss 8.8810e+00 (8.8810e+00)\tAcc@1   4.69 (  4.69)\tAcc@5   7.81 (  7.81)\n","Epoch: [56][0/3]\tTime  0.671 ( 0.671)\tData  0.235 ( 0.235)\tLoss 8.8817e+00 (8.8817e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  10.16 ( 10.16)\n","Epoch: [57][0/3]\tTime  0.690 ( 0.690)\tData  0.236 ( 0.236)\tLoss 8.4818e+00 (8.4818e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   7.81 (  7.81)\n","Epoch: [58][0/3]\tTime  0.686 ( 0.686)\tData  0.241 ( 0.241)\tLoss 8.4198e+00 (8.4198e+00)\tAcc@1   7.81 (  7.81)\tAcc@5  11.72 ( 11.72)\n","Epoch: [59][0/3]\tTime  0.685 ( 0.685)\tData  0.242 ( 0.242)\tLoss 8.3268e+00 (8.3268e+00)\tAcc@1   3.91 (  3.91)\tAcc@5  14.06 ( 14.06)\n","Epoch: [60][0/3]\tTime  0.716 ( 0.716)\tData  0.244 ( 0.244)\tLoss 8.9918e+00 (8.9918e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  14.84 ( 14.84)\n","Epoch: [61][0/3]\tTime  0.935 ( 0.935)\tData  0.331 ( 0.331)\tLoss 8.6404e+00 (8.6404e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  10.94 ( 10.94)\n","Epoch: [62][0/3]\tTime  1.090 ( 1.090)\tData  0.347 ( 0.347)\tLoss 8.7167e+00 (8.7167e+00)\tAcc@1   6.25 (  6.25)\tAcc@5  13.28 ( 13.28)\n","Epoch: [63][0/3]\tTime  0.678 ( 0.678)\tData  0.245 ( 0.245)\tLoss 8.0731e+00 (8.0731e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  11.72 ( 11.72)\n","Epoch: [64][0/3]\tTime  0.694 ( 0.694)\tData  0.237 ( 0.237)\tLoss 8.3557e+00 (8.3557e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  10.16 ( 10.16)\n","Epoch: [65][0/3]\tTime  0.682 ( 0.682)\tData  0.241 ( 0.241)\tLoss 8.0783e+00 (8.0783e+00)\tAcc@1  11.72 ( 11.72)\tAcc@5  17.19 ( 17.19)\n","Epoch: [66][0/3]\tTime  0.669 ( 0.669)\tData  0.230 ( 0.230)\tLoss 8.1761e+00 (8.1761e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  10.94 ( 10.94)\n","Epoch: [67][0/3]\tTime  0.689 ( 0.689)\tData  0.240 ( 0.240)\tLoss 8.4962e+00 (8.4962e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  10.94 ( 10.94)\n","Epoch: [68][0/3]\tTime  0.670 ( 0.670)\tData  0.233 ( 0.233)\tLoss 7.9533e+00 (7.9533e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  10.16 ( 10.16)\n","Epoch: [69][0/3]\tTime  0.770 ( 0.770)\tData  0.245 ( 0.245)\tLoss 8.1394e+00 (8.1394e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   7.81 (  7.81)\n","Epoch: [70][0/3]\tTime  1.031 ( 1.031)\tData  0.370 ( 0.370)\tLoss 7.8515e+00 (7.8515e+00)\tAcc@1   7.81 (  7.81)\tAcc@5  20.31 ( 20.31)\n","Epoch: [71][0/3]\tTime  0.863 ( 0.863)\tData  0.364 ( 0.364)\tLoss 8.0805e+00 (8.0805e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   7.03 (  7.03)\n","Epoch: [72][0/3]\tTime  0.692 ( 0.692)\tData  0.238 ( 0.238)\tLoss 8.0857e+00 (8.0857e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   8.59 (  8.59)\n","Epoch: [73][0/3]\tTime  0.685 ( 0.685)\tData  0.241 ( 0.241)\tLoss 8.2308e+00 (8.2308e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   7.81 (  7.81)\n","Epoch: [74][0/3]\tTime  0.685 ( 0.685)\tData  0.237 ( 0.237)\tLoss 8.0757e+00 (8.0757e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  12.50 ( 12.50)\n","Epoch: [75][0/3]\tTime  0.695 ( 0.695)\tData  0.247 ( 0.247)\tLoss 8.3445e+00 (8.3445e+00)\tAcc@1   8.59 (  8.59)\tAcc@5  14.84 ( 14.84)\n","Epoch: [76][0/3]\tTime  0.700 ( 0.700)\tData  0.239 ( 0.239)\tLoss 8.0140e+00 (8.0140e+00)\tAcc@1   9.38 (  9.38)\tAcc@5  18.75 ( 18.75)\n","Epoch: [77][0/3]\tTime  0.684 ( 0.684)\tData  0.248 ( 0.248)\tLoss 8.1309e+00 (8.1309e+00)\tAcc@1  10.16 ( 10.16)\tAcc@5  21.88 ( 21.88)\n","Epoch: [78][0/3]\tTime  1.009 ( 1.009)\tData  0.346 ( 0.346)\tLoss 7.8579e+00 (7.8579e+00)\tAcc@1   6.25 (  6.25)\tAcc@5  16.41 ( 16.41)\n","Epoch: [79][0/3]\tTime  1.043 ( 1.043)\tData  0.346 ( 0.346)\tLoss 7.8798e+00 (7.8798e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  14.84 ( 14.84)\n","Epoch: [80][0/3]\tTime  0.666 ( 0.666)\tData  0.236 ( 0.236)\tLoss 7.7377e+00 (7.7377e+00)\tAcc@1   6.25 (  6.25)\tAcc@5  16.41 ( 16.41)\n","Epoch: [81][0/3]\tTime  0.696 ( 0.696)\tData  0.237 ( 0.237)\tLoss 7.5642e+00 (7.5642e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  11.72 ( 11.72)\n","Epoch: [82][0/3]\tTime  0.695 ( 0.695)\tData  0.251 ( 0.251)\tLoss 7.8578e+00 (7.8578e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  15.62 ( 15.62)\n","Epoch: [83][0/3]\tTime  0.674 ( 0.674)\tData  0.235 ( 0.235)\tLoss 7.4764e+00 (7.4764e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  15.62 ( 15.62)\n","Epoch: [84][0/3]\tTime  0.704 ( 0.704)\tData  0.270 ( 0.270)\tLoss 7.6363e+00 (7.6363e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  13.28 ( 13.28)\n","Epoch: [85][0/3]\tTime  0.691 ( 0.691)\tData  0.237 ( 0.237)\tLoss 7.7307e+00 (7.7307e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  11.72 ( 11.72)\n","Epoch: [86][0/3]\tTime  0.688 ( 0.688)\tData  0.256 ( 0.256)\tLoss 7.6477e+00 (7.6477e+00)\tAcc@1   6.25 (  6.25)\tAcc@5  10.16 ( 10.16)\n","Epoch: [87][0/3]\tTime  0.985 ( 0.985)\tData  0.323 ( 0.323)\tLoss 7.5455e+00 (7.5455e+00)\tAcc@1   7.81 (  7.81)\tAcc@5  21.09 ( 21.09)\n","Epoch: [88][0/3]\tTime  1.128 ( 1.128)\tData  0.348 ( 0.348)\tLoss 7.7454e+00 (7.7454e+00)\tAcc@1   6.25 (  6.25)\tAcc@5  10.16 ( 10.16)\n","Epoch: [89][0/3]\tTime  0.664 ( 0.664)\tData  0.235 ( 0.235)\tLoss 7.7140e+00 (7.7140e+00)\tAcc@1   8.59 (  8.59)\tAcc@5  19.53 ( 19.53)\n","Epoch: [90][0/3]\tTime  0.690 ( 0.690)\tData  0.242 ( 0.242)\tLoss 7.4855e+00 (7.4855e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   8.59 (  8.59)\n","Epoch: [91][0/3]\tTime  0.674 ( 0.674)\tData  0.242 ( 0.242)\tLoss 7.5694e+00 (7.5694e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  10.16 ( 10.16)\n","Epoch: [92][0/3]\tTime  0.677 ( 0.677)\tData  0.236 ( 0.236)\tLoss 7.6352e+00 (7.6352e+00)\tAcc@1  14.84 ( 14.84)\tAcc@5  23.44 ( 23.44)\n","Epoch: [93][0/3]\tTime  0.688 ( 0.688)\tData  0.240 ( 0.240)\tLoss 7.3525e+00 (7.3525e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  15.62 ( 15.62)\n","Epoch: [94][0/3]\tTime  0.672 ( 0.672)\tData  0.241 ( 0.241)\tLoss 7.2931e+00 (7.2931e+00)\tAcc@1  16.41 ( 16.41)\tAcc@5  25.78 ( 25.78)\n","Epoch: [95][0/3]\tTime  0.714 ( 0.714)\tData  0.239 ( 0.239)\tLoss 7.4024e+00 (7.4024e+00)\tAcc@1   8.59 (  8.59)\tAcc@5  16.41 ( 16.41)\n","Epoch: [96][0/3]\tTime  0.965 ( 0.965)\tData  0.338 ( 0.338)\tLoss 7.3482e+00 (7.3482e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  17.19 ( 17.19)\n","Epoch: [97][0/3]\tTime  1.118 ( 1.118)\tData  0.379 ( 0.379)\tLoss 7.3815e+00 (7.3815e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  14.06 ( 14.06)\n","Epoch: [98][0/3]\tTime  0.691 ( 0.691)\tData  0.253 ( 0.253)\tLoss 7.4880e+00 (7.4880e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   2.34 (  2.34)\n","Epoch: [99][0/3]\tTime  0.666 ( 0.666)\tData  0.236 ( 0.236)\tLoss 7.6016e+00 (7.6016e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   3.91 (  3.91)\n","Epoch: [100][0/3]\tTime  0.684 ( 0.684)\tData  0.243 ( 0.243)\tLoss 7.2990e+00 (7.2990e+00)\tAcc@1   6.25 (  6.25)\tAcc@5  20.31 ( 20.31)\n","Epoch: [101][0/3]\tTime  0.669 ( 0.669)\tData  0.240 ( 0.240)\tLoss 7.2390e+00 (7.2390e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  17.19 ( 17.19)\n","Epoch: [102][0/3]\tTime  0.682 ( 0.682)\tData  0.233 ( 0.233)\tLoss 7.0976e+00 (7.0976e+00)\tAcc@1   3.91 (  3.91)\tAcc@5  13.28 ( 13.28)\n","Epoch: [103][0/3]\tTime  0.682 ( 0.682)\tData  0.249 ( 0.249)\tLoss 7.1634e+00 (7.1634e+00)\tAcc@1  14.84 ( 14.84)\tAcc@5  21.88 ( 21.88)\n","Epoch: [104][0/3]\tTime  0.879 ( 0.879)\tData  0.250 ( 0.250)\tLoss 7.2570e+00 (7.2570e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  10.94 ( 10.94)\n","Epoch: [105][0/3]\tTime  1.111 ( 1.111)\tData  0.337 ( 0.337)\tLoss 7.3387e+00 (7.3387e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  13.28 ( 13.28)\n","Epoch: [106][0/3]\tTime  0.782 ( 0.782)\tData  0.344 ( 0.344)\tLoss 7.2647e+00 (7.2647e+00)\tAcc@1   9.38 (  9.38)\tAcc@5  17.19 ( 17.19)\n","Epoch: [107][0/3]\tTime  0.681 ( 0.681)\tData  0.235 ( 0.235)\tLoss 7.4760e+00 (7.4760e+00)\tAcc@1  19.53 ( 19.53)\tAcc@5  31.25 ( 31.25)\n","Epoch: [108][0/3]\tTime  0.669 ( 0.669)\tData  0.233 ( 0.233)\tLoss 7.0716e+00 (7.0716e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  13.28 ( 13.28)\n","Epoch: [109][0/3]\tTime  0.695 ( 0.695)\tData  0.237 ( 0.237)\tLoss 7.1887e+00 (7.1887e+00)\tAcc@1   3.91 (  3.91)\tAcc@5  11.72 ( 11.72)\n","Epoch: [110][0/3]\tTime  0.682 ( 0.682)\tData  0.243 ( 0.243)\tLoss 7.4067e+00 (7.4067e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   3.91 (  3.91)\n","Epoch: [111][0/3]\tTime  0.694 ( 0.694)\tData  0.235 ( 0.235)\tLoss 7.2113e+00 (7.2113e+00)\tAcc@1   9.38 (  9.38)\tAcc@5  25.78 ( 25.78)\n","Epoch: [112][0/3]\tTime  0.692 ( 0.692)\tData  0.251 ( 0.251)\tLoss 7.0889e+00 (7.0889e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  10.16 ( 10.16)\n","Epoch: [113][0/3]\tTime  1.020 ( 1.020)\tData  0.349 ( 0.349)\tLoss 7.3658e+00 (7.3658e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   9.38 (  9.38)\n","Epoch: [114][0/3]\tTime  1.071 ( 1.071)\tData  0.329 ( 0.329)\tLoss 7.2748e+00 (7.2748e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   8.59 (  8.59)\n","Epoch: [115][0/3]\tTime  0.692 ( 0.692)\tData  0.248 ( 0.248)\tLoss 7.2127e+00 (7.2127e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  16.41 ( 16.41)\n","Epoch: [116][0/3]\tTime  0.687 ( 0.687)\tData  0.242 ( 0.242)\tLoss 6.9468e+00 (6.9468e+00)\tAcc@1   7.81 (  7.81)\tAcc@5  16.41 ( 16.41)\n","Epoch: [117][0/3]\tTime  0.724 ( 0.724)\tData  0.294 ( 0.294)\tLoss 7.0435e+00 (7.0435e+00)\tAcc@1   3.91 (  3.91)\tAcc@5  12.50 ( 12.50)\n","Epoch: [118][0/3]\tTime  0.686 ( 0.686)\tData  0.243 ( 0.243)\tLoss 7.1449e+00 (7.1449e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  11.72 ( 11.72)\n","Epoch: [119][0/3]\tTime  0.701 ( 0.701)\tData  0.257 ( 0.257)\tLoss 6.9488e+00 (6.9488e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  21.09 ( 21.09)\n","Epoch: [120][0/3]\tTime  0.685 ( 0.685)\tData  0.241 ( 0.241)\tLoss 6.9209e+00 (6.9209e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  11.72 ( 11.72)\n","Epoch: [121][0/3]\tTime  0.691 ( 0.691)\tData  0.251 ( 0.251)\tLoss 7.1365e+00 (7.1365e+00)\tAcc@1   7.81 (  7.81)\tAcc@5  22.66 ( 22.66)\n","Epoch: [122][0/3]\tTime  0.948 ( 0.948)\tData  0.340 ( 0.340)\tLoss 6.8152e+00 (6.8152e+00)\tAcc@1  10.16 ( 10.16)\tAcc@5  25.78 ( 25.78)\n","Epoch: [123][0/3]\tTime  0.910 ( 0.910)\tData  0.350 ( 0.350)\tLoss 7.2495e+00 (7.2495e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   3.12 (  3.12)\n","Epoch: [124][0/3]\tTime  0.684 ( 0.684)\tData  0.247 ( 0.247)\tLoss 6.9205e+00 (6.9205e+00)\tAcc@1   9.38 (  9.38)\tAcc@5  21.09 ( 21.09)\n","Epoch: [125][0/3]\tTime  0.676 ( 0.676)\tData  0.236 ( 0.236)\tLoss 7.0574e+00 (7.0574e+00)\tAcc@1   3.91 (  3.91)\tAcc@5   6.25 (  6.25)\n","Epoch: [126][0/3]\tTime  0.679 ( 0.679)\tData  0.230 ( 0.230)\tLoss 7.3414e+00 (7.3414e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   7.03 (  7.03)\n","Epoch: [127][0/3]\tTime  0.677 ( 0.677)\tData  0.239 ( 0.239)\tLoss 6.7241e+00 (6.7241e+00)\tAcc@1  17.97 ( 17.97)\tAcc@5  29.69 ( 29.69)\n","Epoch: [128][0/3]\tTime  0.685 ( 0.685)\tData  0.237 ( 0.237)\tLoss 7.0751e+00 (7.0751e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   5.47 (  5.47)\n","Epoch: [129][0/3]\tTime  0.674 ( 0.674)\tData  0.237 ( 0.237)\tLoss 7.2891e+00 (7.2891e+00)\tAcc@1   6.25 (  6.25)\tAcc@5  20.31 ( 20.31)\n","Epoch: [130][0/3]\tTime  0.756 ( 0.756)\tData  0.240 ( 0.240)\tLoss 7.1126e+00 (7.1126e+00)\tAcc@1  10.94 ( 10.94)\tAcc@5  21.09 ( 21.09)\n","Epoch: [131][0/3]\tTime  1.071 ( 1.071)\tData  0.382 ( 0.382)\tLoss 7.1658e+00 (7.1658e+00)\tAcc@1  13.28 ( 13.28)\tAcc@5  21.09 ( 21.09)\n","Epoch: [132][0/3]\tTime  0.842 ( 0.842)\tData  0.400 ( 0.400)\tLoss 7.4529e+00 (7.4529e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   7.03 (  7.03)\n","Epoch: [133][0/3]\tTime  0.674 ( 0.674)\tData  0.234 ( 0.234)\tLoss 7.1177e+00 (7.1177e+00)\tAcc@1   9.38 (  9.38)\tAcc@5  22.66 ( 22.66)\n","Epoch: [134][0/3]\tTime  0.694 ( 0.694)\tData  0.258 ( 0.258)\tLoss 7.3622e+00 (7.3622e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   7.03 (  7.03)\n","Epoch: [135][0/3]\tTime  0.665 ( 0.665)\tData  0.230 ( 0.230)\tLoss 6.8701e+00 (6.8701e+00)\tAcc@1  13.28 ( 13.28)\tAcc@5  25.78 ( 25.78)\n","Epoch: [136][0/3]\tTime  0.711 ( 0.711)\tData  0.250 ( 0.250)\tLoss 7.1923e+00 (7.1923e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   8.59 (  8.59)\n","Epoch: [137][0/3]\tTime  0.662 ( 0.662)\tData  0.234 ( 0.234)\tLoss 7.1607e+00 (7.1607e+00)\tAcc@1  10.16 ( 10.16)\tAcc@5  25.00 ( 25.00)\n","Epoch: [138][0/3]\tTime  0.700 ( 0.700)\tData  0.232 ( 0.232)\tLoss 7.1955e+00 (7.1955e+00)\tAcc@1   3.91 (  3.91)\tAcc@5  10.94 ( 10.94)\n","Epoch: [139][0/3]\tTime  0.940 ( 0.940)\tData  0.337 ( 0.337)\tLoss 7.1571e+00 (7.1571e+00)\tAcc@1  11.72 ( 11.72)\tAcc@5  25.00 ( 25.00)\n","Epoch: [140][0/3]\tTime  1.006 ( 1.006)\tData  0.343 ( 0.343)\tLoss 6.8334e+00 (6.8334e+00)\tAcc@1   7.81 (  7.81)\tAcc@5  18.75 ( 18.75)\n","Epoch: [141][0/3]\tTime  0.681 ( 0.681)\tData  0.244 ( 0.244)\tLoss 7.0842e+00 (7.0842e+00)\tAcc@1   7.81 (  7.81)\tAcc@5  14.84 ( 14.84)\n","Epoch: [142][0/3]\tTime  0.675 ( 0.675)\tData  0.235 ( 0.235)\tLoss 7.0828e+00 (7.0828e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   9.38 (  9.38)\n","Epoch: [143][0/3]\tTime  0.689 ( 0.689)\tData  0.238 ( 0.238)\tLoss 7.4754e+00 (7.4754e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   7.81 (  7.81)\n","Epoch: [144][0/3]\tTime  0.669 ( 0.669)\tData  0.231 ( 0.231)\tLoss 7.4961e+00 (7.4961e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   4.69 (  4.69)\n","Epoch: [145][0/3]\tTime  0.679 ( 0.679)\tData  0.241 ( 0.241)\tLoss 7.2861e+00 (7.2861e+00)\tAcc@1  10.94 ( 10.94)\tAcc@5  19.53 ( 19.53)\n","Epoch: [146][0/3]\tTime  0.679 ( 0.679)\tData  0.245 ( 0.245)\tLoss 7.1687e+00 (7.1687e+00)\tAcc@1   3.91 (  3.91)\tAcc@5  17.97 ( 17.97)\n","Epoch: [147][0/3]\tTime  0.669 ( 0.669)\tData  0.233 ( 0.233)\tLoss 7.0470e+00 (7.0470e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  11.72 ( 11.72)\n","Epoch: [148][0/3]\tTime  0.968 ( 0.968)\tData  0.342 ( 0.342)\tLoss 7.6148e+00 (7.6148e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   7.03 (  7.03)\n","Epoch: [149][0/3]\tTime  1.033 ( 1.033)\tData  0.371 ( 0.371)\tLoss 7.3212e+00 (7.3212e+00)\tAcc@1  10.16 ( 10.16)\tAcc@5  24.22 ( 24.22)\n","Epoch: [150][0/3]\tTime  0.689 ( 0.689)\tData  0.243 ( 0.243)\tLoss 7.6961e+00 (7.6961e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   8.59 (  8.59)\n","Epoch: [151][0/3]\tTime  0.690 ( 0.690)\tData  0.250 ( 0.250)\tLoss 7.6140e+00 (7.6140e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   6.25 (  6.25)\n","Epoch: [152][0/3]\tTime  0.694 ( 0.694)\tData  0.235 ( 0.235)\tLoss 7.2081e+00 (7.2081e+00)\tAcc@1   3.91 (  3.91)\tAcc@5  18.75 ( 18.75)\n","Epoch: [153][0/3]\tTime  0.688 ( 0.688)\tData  0.248 ( 0.248)\tLoss 7.4408e+00 (7.4408e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  10.16 ( 10.16)\n","Epoch: [154][0/3]\tTime  0.666 ( 0.666)\tData  0.233 ( 0.233)\tLoss 7.3594e+00 (7.3594e+00)\tAcc@1  10.16 ( 10.16)\tAcc@5  19.53 ( 19.53)\n","Epoch: [155][0/3]\tTime  0.681 ( 0.681)\tData  0.248 ( 0.248)\tLoss 7.1824e+00 (7.1824e+00)\tAcc@1  10.94 ( 10.94)\tAcc@5  20.31 ( 20.31)\n","Epoch: [156][0/3]\tTime  0.791 ( 0.791)\tData  0.272 ( 0.272)\tLoss 7.7056e+00 (7.7056e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.78 (  0.78)\n","Epoch: [157][0/3]\tTime  0.983 ( 0.983)\tData  0.361 ( 0.361)\tLoss 6.9787e+00 (6.9787e+00)\tAcc@1  13.28 ( 13.28)\tAcc@5  27.34 ( 27.34)\n","Epoch: [158][0/3]\tTime  0.959 ( 0.959)\tData  0.367 ( 0.367)\tLoss 7.5906e+00 (7.5906e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   5.47 (  5.47)\n","Epoch: [159][0/3]\tTime  0.697 ( 0.697)\tData  0.243 ( 0.243)\tLoss 7.4290e+00 (7.4290e+00)\tAcc@1   9.38 (  9.38)\tAcc@5  21.88 ( 21.88)\n","Epoch: [160][0/3]\tTime  0.684 ( 0.684)\tData  0.248 ( 0.248)\tLoss 7.2184e+00 (7.2184e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  16.41 ( 16.41)\n","Epoch: [161][0/3]\tTime  0.674 ( 0.674)\tData  0.238 ( 0.238)\tLoss 7.2645e+00 (7.2645e+00)\tAcc@1   3.91 (  3.91)\tAcc@5  10.16 ( 10.16)\n","Epoch: [162][0/3]\tTime  0.697 ( 0.697)\tData  0.242 ( 0.242)\tLoss 7.2618e+00 (7.2618e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   9.38 (  9.38)\n","Epoch: [163][0/3]\tTime  0.665 ( 0.665)\tData  0.235 ( 0.235)\tLoss 7.2219e+00 (7.2219e+00)\tAcc@1   8.59 (  8.59)\tAcc@5  16.41 ( 16.41)\n","Epoch: [164][0/3]\tTime  0.689 ( 0.689)\tData  0.233 ( 0.233)\tLoss 7.8120e+00 (7.8120e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   3.91 (  3.91)\n","Epoch: [165][0/3]\tTime  1.098 ( 1.098)\tData  0.358 ( 0.358)\tLoss 7.5765e+00 (7.5765e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  11.72 ( 11.72)\n","Epoch: [166][0/3]\tTime  0.981 ( 0.981)\tData  0.343 ( 0.343)\tLoss 7.4589e+00 (7.4589e+00)\tAcc@1   6.25 (  6.25)\tAcc@5  15.62 ( 15.62)\n","Epoch: [167][0/3]\tTime  0.692 ( 0.692)\tData  0.248 ( 0.248)\tLoss 7.2499e+00 (7.2499e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  17.97 ( 17.97)\n","Epoch: [168][0/3]\tTime  0.678 ( 0.678)\tData  0.243 ( 0.243)\tLoss 7.6931e+00 (7.6931e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  10.16 ( 10.16)\n","Epoch: [169][0/3]\tTime  0.704 ( 0.704)\tData  0.244 ( 0.244)\tLoss 7.5941e+00 (7.5941e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   5.47 (  5.47)\n","Epoch: [170][0/3]\tTime  0.668 ( 0.668)\tData  0.238 ( 0.238)\tLoss 7.3874e+00 (7.3874e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   7.81 (  7.81)\n","Epoch: [171][0/3]\tTime  0.688 ( 0.688)\tData  0.234 ( 0.234)\tLoss 7.7158e+00 (7.7158e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   7.03 (  7.03)\n","Epoch: [172][0/3]\tTime  0.671 ( 0.671)\tData  0.238 ( 0.238)\tLoss 7.7305e+00 (7.7305e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   3.91 (  3.91)\n","Epoch: [173][0/3]\tTime  0.677 ( 0.677)\tData  0.229 ( 0.229)\tLoss 7.3981e+00 (7.3981e+00)\tAcc@1  11.72 ( 11.72)\tAcc@5  24.22 ( 24.22)\n","Epoch: [174][0/3]\tTime  1.003 ( 1.003)\tData  0.359 ( 0.359)\tLoss 7.4320e+00 (7.4320e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  17.97 ( 17.97)\n","Epoch: [175][0/3]\tTime  0.978 ( 0.978)\tData  0.353 ( 0.353)\tLoss 7.6028e+00 (7.6028e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  13.28 ( 13.28)\n","Epoch: [176][0/3]\tTime  0.687 ( 0.687)\tData  0.248 ( 0.248)\tLoss 7.7906e+00 (7.7906e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   2.34 (  2.34)\n","Epoch: [177][0/3]\tTime  0.676 ( 0.676)\tData  0.242 ( 0.242)\tLoss 7.9789e+00 (7.9789e+00)\tAcc@1   1.56 (  1.56)\tAcc@5  11.72 ( 11.72)\n","Epoch: [178][0/3]\tTime  0.678 ( 0.678)\tData  0.243 ( 0.243)\tLoss 7.7342e+00 (7.7342e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   5.47 (  5.47)\n","Epoch: [179][0/3]\tTime  0.676 ( 0.676)\tData  0.243 ( 0.243)\tLoss 7.4263e+00 (7.4263e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  15.62 ( 15.62)\n","Epoch: [180][0/3]\tTime  0.672 ( 0.672)\tData  0.235 ( 0.235)\tLoss 7.6558e+00 (7.6558e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   2.34 (  2.34)\n","Epoch: [181][0/3]\tTime  0.675 ( 0.675)\tData  0.237 ( 0.237)\tLoss 7.4643e+00 (7.4643e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  17.19 ( 17.19)\n","Epoch: [182][0/3]\tTime  0.680 ( 0.680)\tData  0.234 ( 0.234)\tLoss 7.6088e+00 (7.6088e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   9.38 (  9.38)\n","Epoch: [183][0/3]\tTime  0.922 ( 0.922)\tData  0.323 ( 0.323)\tLoss 7.7328e+00 (7.7328e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   6.25 (  6.25)\n","Epoch: [184][0/3]\tTime  1.006 ( 1.006)\tData  0.351 ( 0.351)\tLoss 7.8991e+00 (7.8991e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   3.12 (  3.12)\n","Epoch: [185][0/3]\tTime  0.680 ( 0.680)\tData  0.249 ( 0.249)\tLoss 7.9453e+00 (7.9453e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   3.91 (  3.91)\n","Epoch: [186][0/3]\tTime  0.684 ( 0.684)\tData  0.232 ( 0.232)\tLoss 8.0869e+00 (8.0869e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  12.50 ( 12.50)\n","Epoch: [187][0/3]\tTime  0.671 ( 0.671)\tData  0.243 ( 0.243)\tLoss 7.9997e+00 (7.9997e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   1.56 (  1.56)\n","Epoch: [188][0/3]\tTime  0.701 ( 0.701)\tData  0.250 ( 0.250)\tLoss 7.9307e+00 (7.9307e+00)\tAcc@1   0.78 (  0.78)\tAcc@5   2.34 (  2.34)\n","Epoch: [189][0/3]\tTime  0.698 ( 0.698)\tData  0.264 ( 0.264)\tLoss 8.2420e+00 (8.2420e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   3.91 (  3.91)\n","Epoch: [190][0/3]\tTime  0.693 ( 0.693)\tData  0.243 ( 0.243)\tLoss 7.9937e+00 (7.9937e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  10.16 ( 10.16)\n","Epoch: [191][0/3]\tTime  0.771 ( 0.771)\tData  0.248 ( 0.248)\tLoss 8.1927e+00 (8.1927e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   3.91 (  3.91)\n","Epoch: [192][0/3]\tTime  0.953 ( 0.953)\tData  0.343 ( 0.343)\tLoss 8.0883e+00 (8.0883e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  16.41 ( 16.41)\n","Epoch: [193][0/3]\tTime  0.804 ( 0.804)\tData  0.360 ( 0.360)\tLoss 7.4767e+00 (7.4767e+00)\tAcc@1   2.34 (  2.34)\tAcc@5  14.06 ( 14.06)\n","Epoch: [194][0/3]\tTime  0.673 ( 0.673)\tData  0.239 ( 0.239)\tLoss 7.6575e+00 (7.6575e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  13.28 ( 13.28)\n","Epoch: [195][0/3]\tTime  0.680 ( 0.680)\tData  0.235 ( 0.235)\tLoss 7.6209e+00 (7.6209e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  17.19 ( 17.19)\n","Epoch: [196][0/3]\tTime  0.688 ( 0.688)\tData  0.251 ( 0.251)\tLoss 7.6942e+00 (7.6942e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  14.06 ( 14.06)\n","Epoch: [197][0/3]\tTime  0.704 ( 0.704)\tData  0.244 ( 0.244)\tLoss 8.0952e+00 (8.0952e+00)\tAcc@1   3.12 (  3.12)\tAcc@5  11.72 ( 11.72)\n","Epoch: [198][0/3]\tTime  0.687 ( 0.687)\tData  0.256 ( 0.256)\tLoss 7.8396e+00 (7.8396e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   7.03 (  7.03)\n","Epoch: [199][0/3]\tTime  0.693 ( 0.693)\tData  0.257 ( 0.257)\tLoss 7.9673e+00 (7.9673e+00)\tAcc@1   4.69 (  4.69)\tAcc@5  14.06 ( 14.06)\n"]}],"source":["main()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"drvwPZhFBfNU","executionInfo":{"status":"ok","timestamp":1718481756669,"user_tz":-60,"elapsed":231,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"}}},"outputs":[],"source":["#!/usr/bin/env python\n","# Copyright (c) Meta Platforms, Inc. and affiliates.\n","\n","# This source code is licensed under the MIT license found in the\n","# LICENSE file in the root directory of this source tree.\n","\n","class LinClrDataset(Dataset):\n","    def __init__(self, file_path, mode='train', transform=None):\n","        data = np.load(file_path)\n","        self.images = data[f'{mode}_images']\n","        self.labels = data[f'{mode}_labels']  # Note: Labels might not be used in MoCo training\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        if self.transform:\n","            q = self.transform(image)\n","        labels = self.labels[idx]\n","        return q, labels\n","\n","\n","class CustomImageClassifier(nn.Module):\n","    def __init__(self, num_classes, dim):\n","        super().__init__()\n","        self.encoder = ImageEncoder()\n","        self.dim = dim\n","        # Assume the output features of ImageEncoder are 2048-dimensional\n","        self.fc = nn.Linear(self.dim, num_classes)\n","\n","    def forward(self, x):\n","        features = self.encoder(x)\n","        output = self.fc(features)\n","        return output\n","\n","\n","model_names = sorted(\n","    name\n","    for name in models.__dict__\n","    if name.islower() and not name.startswith(\"__\") and callable(models.__dict__[name])\n",")\n","\n","class Args:\n","    data = \"/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/MoCo\"  # path to dataset\n","    arch = \"resnet50\" #model architecture, default: resnet50\n","    workers = 1 #number of loading workers, def:32\n","    epochs = 100 #total numnber of epochs, def: 100\n","    start_epoch = 0 #(for restarts)\n","    batch_size = 128 #mini-batch size\n","    lr = 30.0 #initial learning rate\n","    schedule = [60, 80] #learning rate schedule, when to drop lr by a ratio\n","    momentum = 0.9\n","    weight_decay = 0.0\n","    print_freq = 10\n","    resume = \"\" #path to latest checkpoint\n","    evaluate = False #evaluate model on validation set\n","    world_size = -1 #number of nodes for distributed training\n","    rank = -1 #node rank for distrivuted training\n","    dist_url = \"tcp://224.66.41.62:23456\" #url used for distributed training\n","    dist_backend = \"nccl\" #distributed backend\n","    seed = None\n","    gpu = 0\n","    multiprocessing_distributed = False\n","    pretrained = \"/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/MoCo/checkpoint_0199.pth.tar\" #path to moco pretrained checkpoint\n","    size = 24\n","    num_classes = 6 #adjust per dataset\n","\n","    # Custom additions for MoCo-specific configs\n","    moco_dim = 2048\n","    moco_k = 65536\n","    moco_m = 0.999\n","    moco_t = 0.07\n","    mlp = False\n","    aug_plus = True\n","    cos = False\n","\n","# Usage\n","args = Args()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","best_acc1 = 0\n","\n","\n","def linclsmain():\n","\n","    if args.seed is not None:\n","        random.seed(args.seed)\n","        torch.manual_seed(args.seed)\n","        cudnn.deterministic = True\n","        warnings.warn(\n","            \"You have chosen to seed training. \"\n","            \"This will turn on the CUDNN deterministic setting, \"\n","            \"which can slow down your training considerably! \"\n","            \"You may see unexpected behavior when restarting \"\n","            \"from checkpoints.\"\n","        )\n","\n","    if args.gpu is not None:\n","        warnings.warn(\n","            \"You have chosen a specific GPU. This will completely \"\n","            \"disable data parallelism.\"\n","        )\n","\n","    if args.dist_url == \"env://\" and args.world_size == -1:\n","        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n","\n","    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n","\n","    ngpus_per_node = torch.cuda.device_count()\n","\n","    main_worker(args.gpu, ngpus_per_node, args)\n","\n","\n","def main_worker(gpu, ngpus_per_node, args):\n","    global best_acc1\n","    args.gpu = gpu\n","\n","    # suppress printing if not master\n","    if args.multiprocessing_distributed and args.gpu != 0:\n","\n","        def print_pass(*args):\n","            pass\n","\n","        builtins.print = print_pass\n","\n","    if args.gpu is not None:\n","        print(\"Use GPU: {} for training\".format(args.gpu))\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # create model\n","    print(\"=> creating model '{}'\".format(args.arch))\n","    model = CustomImageClassifier(num_classes = args.num_classes, dim = args.moco_dim)\n","    model.to(device)\n","\n","    # freeze all layers but the last fc\n","    for name, param in model.named_parameters():\n","        if name not in [\"fc.weight\", \"fc.bias\"]:\n","            param.requires_grad = False\n","    # init the fc layer\n","    model.fc.weight.data.normal_(mean=0.0, std=0.01)\n","    model.fc.bias.data.zero_()\n","\n","    if args.pretrained:\n","      if os.path.isfile(args.pretrained):\n","        print(\"=> loading checkpoint '{}'\".format(args.pretrained))\n","        checkpoint = torch.load(args.pretrained, map_location=\"cpu\")\n","        state_dict = checkpoint[\"state_dict\"]\n","\n","        # Print keys to debug\n","        print(\"Keys in the loaded state_dict:\", state_dict.keys())\n","\n","        # Create a new state dictionary with adjusted keys\n","        new_state_dict = {}\n","        for k, v in state_dict.items():\n","            new_key = k.replace(\"module.encoder_q.\", \"\")  # Adjust based on actual prefix and structure\n","            if \"fc\" not in new_key:  # Ignore fully connected layers if they should not be loaded\n","                new_state_dict[new_key] = v\n","\n","        # Load the new state dictionary\n","        model.load_state_dict(new_state_dict, strict=False)\n","        print(\"=> loaded pre-trained model '{}'\".format(args.pretrained))\n","      else:\n","        print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n","\n","    if args.distributed:\n","        # For multiprocessing distributed, DistributedDataParallel constructor\n","        # should always set the single device scope, otherwise,\n","        # DistributedDataParallel will use all available devices.\n","        if args.gpu is not None:\n","            model.to(device)\n","            # When using a single GPU per process and per\n","            # DistributedDataParallel, we need to divide the batch size\n","            # ourselves based on the total number of GPUs we have\n","            args.batch_size = int(args.batch_size / ngpus_per_node)\n","            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n","            model = torch.nn.parallel.DistributedDataParallel(\n","                model, device_ids=[args.gpu]\n","            )\n","        else:\n","            model.to(device)\n","            # DistributedDataParallel will divide and allocate batch_size to all\n","            # available GPUs if device_ids are not set\n","            model = torch.nn.parallel.DistributedDataParallel(model)\n","    elif args.gpu is not None:\n","        model = model.to(device)\n","    else:\n","        # DataParallel will divide and allocate batch_size to all available GPUs\n","        if args.arch.startswith(\"alexnet\") or args.arch.startswith(\"vgg\"):\n","            model.features = torch.nn.DataParallel(model.features)\n","            model.to(device)\n","        else:\n","            model = torch.nn.DataParallel(model).to(device)\n","\n","    # define loss function (criterion) and optimizer\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    # optimize only the linear classifier\n","    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n","    assert len(parameters) == 2  # fc.weight, fc.bias\n","    optimizer = torch.optim.SGD(\n","        parameters, args.lr, momentum=args.momentum, weight_decay=args.weight_decay\n","    )\n","\n","    # optionally resume from a checkpoint\n","    if args.resume:\n","        if os.path.isfile(args.resume):\n","            print(\"=> loading checkpoint '{}'\".format(args.resume))\n","            if args.gpu is None:\n","                checkpoint = torch.load(args.resume, map_location=device)\n","            else:\n","                checkpoint = torch.load(args.resume, map_location=device)\n","            args.start_epoch = checkpoint[\"epoch\"]\n","            best_acc1 = checkpoint[\"best_acc1\"]\n","            if args.gpu is not None:\n","                # best_acc1 may be from a checkpoint from a different GPU\n","                best_acc1 = best_acc1.to(args.gpu)\n","            model.load_state_dict(checkpoint[\"state_dict\"])\n","            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","            print(\n","                \"=> loaded checkpoint '{}' (epoch {})\".format(\n","                    args.resume, checkpoint[\"epoch\"]\n","                )\n","            )\n","        else:\n","            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n","\n","    cudnn.benchmark = True\n","\n","    # Data loading code\n","    traindir = os.path.join(args.data, \"train\")\n","    valdir = os.path.join(args.data, \"val\")\n","    normalize = transforms.Normalize(\n","        mean=[0.485], std=[0.229]\n","    )\n","\n","    augmentations = transforms.Compose(\n","            [\n","                transforms.ToPILImage(),\n","                transforms.RandomResizedCrop(args.size),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                normalize,\n","            ]\n","        )\n","\n","\n","    train_dataset = LinClrDataset('/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/SimCLR/ECG5000_processed.npz', mode='train', transform=augmentations)\n","\n","    if args.distributed:\n","        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n","    else:\n","        train_sampler = None\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=args.batch_size,\n","        shuffle=(train_sampler is None),\n","        num_workers=args.workers,\n","        pin_memory=True,\n","        sampler=train_sampler,\n","    )\n","\n","    val_dataset = LinClrDataset('/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/SimCLR/ECG5000_processed.npz', mode='val', transform=augmentations)\n","\n","\n","    val_loader = torch.utils.data.DataLoader(\n","        val_dataset,\n","        batch_size=args.batch_size,\n","        shuffle=False,\n","        num_workers=args.workers,\n","        pin_memory=True,\n","    )\n","\n","    if args.evaluate:\n","        validate(val_loader, model, criterion, args)\n","        return\n","\n","    for epoch in range(args.start_epoch, args.epochs):\n","        if args.distributed:\n","            train_sampler.set_epoch(epoch)\n","        adjust_learning_rate(optimizer, epoch, args)\n","\n","        # train for one epoch\n","        train(train_loader, model, criterion, optimizer, epoch, args)\n","\n","        # evaluate on validation set\n","        acc1 = validate(val_loader, model, criterion, args)\n","\n","        # remember best acc@1 and save checkpoint\n","        is_best = acc1 > best_acc1\n","        best_acc1 = max(acc1, best_acc1)\n","\n","        if not args.multiprocessing_distributed or (\n","            args.multiprocessing_distributed and args.rank % ngpus_per_node == 0\n","        ):\n","            save_checkpoint(\n","                {\n","                    \"epoch\": epoch + 1,\n","                    \"arch\": args.arch,\n","                    \"state_dict\": model.state_dict(),\n","                    \"best_acc1\": best_acc1,\n","                    \"optimizer\": optimizer.state_dict(),\n","                },\n","                is_best,\n","            )\n","            if epoch == args.start_epoch:\n","                sanity_check(model.state_dict(), args.pretrained)\n","\n","\n","def train(train_loader, model, criterion, optimizer, epoch, args):\n","    batch_time = AverageMeter(\"Time\", \":6.3f\")\n","    data_time = AverageMeter(\"Data\", \":6.3f\")\n","    losses = AverageMeter(\"Loss\", \":.4e\")\n","    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n","    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n","    progress = ProgressMeter(\n","        len(train_loader),\n","        [batch_time, data_time, losses, top1, top5],\n","        prefix=\"Epoch: [{}]\".format(epoch),\n","    )\n","\n","    \"\"\"\n","    Switch to eval mode:\n","    Under the protocol of linear classification on frozen features/models,\n","    it is not legitimate to change any part of the pre-trained model.\n","    BatchNorm in train mode may revise running mean/std (even if it receives\n","    no gradient), which are part of the model parameters too.\n","    \"\"\"\n","    model.eval()\n","\n","    end = time.time()\n","    for i, (images, target) in enumerate(train_loader):\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","\n","        if args.gpu is not None:\n","            images = images.to(device)\n","        target = target.to(device)\n","\n","        if target.ndim > 1:\n","          target = target.squeeze()\n","\n","        #print(target.max(), target.min())\n","\n","        # compute output\n","        output = model(images)\n","\n","        loss = criterion(output, target)\n","\n","        # measure accuracy and record loss\n","        #acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        acc1 = accuracy(output, target)\n","        acc5 = acc1\n","        losses.update(loss.item(), images.size(0))\n","        top1.update(acc1, images.size(0))# top1.update(acc1[0], images.size(0))\n","        top5.update(acc5, images.size(0))\n","\n","        # compute gradient and do SGD step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % args.print_freq == 0:\n","            progress.display(i)\n","\n","\n","def validate(val_loader, model, criterion, args):\n","    batch_time = AverageMeter(\"Time\", \":6.3f\")\n","    losses = AverageMeter(\"Loss\", \":.4e\")\n","    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n","    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n","    precision = torchmetrics.Precision(num_classes=args.num_classes, average='weighted', task=\"multiclass\").to(device)\n","    recall = torchmetrics.Recall(num_classes=args.num_classes, average='weighted', task=\"multiclass\").to(device)\n","    f1 = torchmetrics.F1Score(num_classes=args.num_classes, average='weighted', task=\"multiclass\").to(device)\n","\n","    progress = ProgressMeter(\n","        len(val_loader), [batch_time, losses, top1, top5], prefix=\"Test: \"\n","    )\n","\n","    # switch to evaluate mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        end = time.time()\n","        for i, (images, target) in enumerate(val_loader):\n","            images = images.to(device)\n","            target = target.to(device)\n","\n","            # compute output\n","            output = model(images)\n","            loss = criterion(output, target)\n","\n","            # measure accuracy and record loss\n","            #acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","            acc1 = accuracy(output, target)\n","            acc5 = acc1\n","            losses.update(loss.item(), images.size(0))\n","            top1.update(acc1, images.size(0)) # top1.update(acc1[0], images.size(0))\n","            top5.update(acc5, images.size(0))\n","\n","            #update metrics\n","            precision.update(output, target)\n","            recall.update(output, target)\n","            f1.update(output, target)\n","\n","            # measure elapsed time\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","            if i % args.print_freq == 0:\n","                progress.display(i)\n","\n","        # TODO: this should also be done with the ProgressMeter\n","        print(\n","            f\" * Acc@1 {top1.avg:.4f} Acc@5 {top5.avg:.4f} Precision {precision.compute().item():.4f} Recall {recall.compute().item():.4f} F1 Score {f1.compute().item():.4f}\"\n","        )\n","\n","    return top1.avg\n","\n","\n","def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n","    torch.save(state, filename)\n","    if is_best:\n","        shutil.copyfile(filename, \"model_best.pth.tar\")\n","\n","\n","def sanity_check(state_dict, pretrained_weights):\n","    \"\"\"\n","    Linear classifier should not change any weights other than the linear layer.\n","    This sanity check asserts nothing wrong happens (e.g., BN stats updated).\n","    \"\"\"\n","    print(\"=> loading '{}' for sanity check\".format(pretrained_weights))\n","    checkpoint = torch.load(pretrained_weights, map_location=\"cpu\")\n","    state_dict_pre = checkpoint[\"state_dict\"]\n","\n","    for k in list(state_dict.keys()):\n","        # only ignore fc layer\n","        if \"fc.weight\" in k or \"fc.bias\" in k:\n","            continue\n","\n","        # name in pretrained model\n","        if k.startswith(\"module.\"):\n","            k_pre = \"encoder_q.net.\" + k[len(\"module.\"):]  # Adjust based on your print output\n","        else:\n","            k_pre = \"encoder_q.net.\" + k  # Assuming direct match after \"encoder_q.net.\"\n","\n","        # Check if the adjusted key exists in the pre-trained model state dict\n","        if k_pre in state_dict_pre:\n","            assert (state_dict[k].cpu() == state_dict_pre[k_pre]).all(), \"{} is changed in linear classifier training.\".format(k)\n","        else:\n","            print(\"Key not found in pre-trained model:\", k_pre)\n","\n","    print(\"=> sanity check passed.\")\n","\n","\n","class AverageMeter:\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, name, fmt=\":f\"):\n","        self.name = name\n","        self.fmt = fmt\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","    def __str__(self):\n","        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n","        return fmtstr.format(**self.__dict__)\n","\n","\n","class ProgressMeter:\n","    def __init__(self, num_batches, meters, prefix=\"\"):\n","        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n","        self.meters = meters\n","        self.prefix = prefix\n","\n","    def display(self, batch):\n","        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n","        entries += [str(meter) for meter in self.meters]\n","        print(\"\\t\".join(entries))\n","\n","    def _get_batch_fmtstr(self, num_batches):\n","        num_digits = len(str(num_batches // 1))\n","        fmt = \"{:\" + str(num_digits) + \"d}\"\n","        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n","\n","\n","def adjust_learning_rate(optimizer, epoch, args):\n","    \"\"\"Decay the learning rate based on schedule\"\"\"\n","    lr = args.lr\n","    for milestone in args.schedule:\n","        lr *= 0.1 if epoch >= milestone else 1.0\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","'''\n","def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        _, pred = output.topk(maxk, 1, True, True)\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res\n","'''\n","def accuracy(output, target):\n","    pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n","    correct = pred.eq(target.view_as(pred)).sum().item()\n","    return correct / output.shape[0]\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"orAlSlSPF5Id","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718482049354,"user_tz":-60,"elapsed":292914,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"}},"outputId":"9fd87290-fd08-4c3c-e5c4-dfabf8efe371"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-3688689e60e8>:101: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Use GPU: 0 for training\n","=> creating model 'resnet50'\n","=> loading checkpoint '/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/MoCo/checkpoint_0199.pth.tar'\n","Keys in the loaded state_dict: odict_keys(['queue', 'queue_ptr', 'encoder_q.net.conv1.weight', 'encoder_q.net.bn1.weight', 'encoder_q.net.bn1.bias', 'encoder_q.net.bn1.running_mean', 'encoder_q.net.bn1.running_var', 'encoder_q.net.bn1.num_batches_tracked', 'encoder_q.net.layer1.0.conv1.weight', 'encoder_q.net.layer1.0.bn1.weight', 'encoder_q.net.layer1.0.bn1.bias', 'encoder_q.net.layer1.0.bn1.running_mean', 'encoder_q.net.layer1.0.bn1.running_var', 'encoder_q.net.layer1.0.bn1.num_batches_tracked', 'encoder_q.net.layer1.0.conv2.weight', 'encoder_q.net.layer1.0.bn2.weight', 'encoder_q.net.layer1.0.bn2.bias', 'encoder_q.net.layer1.0.bn2.running_mean', 'encoder_q.net.layer1.0.bn2.running_var', 'encoder_q.net.layer1.0.bn2.num_batches_tracked', 'encoder_q.net.layer1.0.conv3.weight', 'encoder_q.net.layer1.0.bn3.weight', 'encoder_q.net.layer1.0.bn3.bias', 'encoder_q.net.layer1.0.bn3.running_mean', 'encoder_q.net.layer1.0.bn3.running_var', 'encoder_q.net.layer1.0.bn3.num_batches_tracked', 'encoder_q.net.layer1.0.downsample.0.weight', 'encoder_q.net.layer1.0.downsample.1.weight', 'encoder_q.net.layer1.0.downsample.1.bias', 'encoder_q.net.layer1.0.downsample.1.running_mean', 'encoder_q.net.layer1.0.downsample.1.running_var', 'encoder_q.net.layer1.0.downsample.1.num_batches_tracked', 'encoder_q.net.layer1.1.conv1.weight', 'encoder_q.net.layer1.1.bn1.weight', 'encoder_q.net.layer1.1.bn1.bias', 'encoder_q.net.layer1.1.bn1.running_mean', 'encoder_q.net.layer1.1.bn1.running_var', 'encoder_q.net.layer1.1.bn1.num_batches_tracked', 'encoder_q.net.layer1.1.conv2.weight', 'encoder_q.net.layer1.1.bn2.weight', 'encoder_q.net.layer1.1.bn2.bias', 'encoder_q.net.layer1.1.bn2.running_mean', 'encoder_q.net.layer1.1.bn2.running_var', 'encoder_q.net.layer1.1.bn2.num_batches_tracked', 'encoder_q.net.layer1.1.conv3.weight', 'encoder_q.net.layer1.1.bn3.weight', 'encoder_q.net.layer1.1.bn3.bias', 'encoder_q.net.layer1.1.bn3.running_mean', 'encoder_q.net.layer1.1.bn3.running_var', 'encoder_q.net.layer1.1.bn3.num_batches_tracked', 'encoder_q.net.layer1.2.conv1.weight', 'encoder_q.net.layer1.2.bn1.weight', 'encoder_q.net.layer1.2.bn1.bias', 'encoder_q.net.layer1.2.bn1.running_mean', 'encoder_q.net.layer1.2.bn1.running_var', 'encoder_q.net.layer1.2.bn1.num_batches_tracked', 'encoder_q.net.layer1.2.conv2.weight', 'encoder_q.net.layer1.2.bn2.weight', 'encoder_q.net.layer1.2.bn2.bias', 'encoder_q.net.layer1.2.bn2.running_mean', 'encoder_q.net.layer1.2.bn2.running_var', 'encoder_q.net.layer1.2.bn2.num_batches_tracked', 'encoder_q.net.layer1.2.conv3.weight', 'encoder_q.net.layer1.2.bn3.weight', 'encoder_q.net.layer1.2.bn3.bias', 'encoder_q.net.layer1.2.bn3.running_mean', 'encoder_q.net.layer1.2.bn3.running_var', 'encoder_q.net.layer1.2.bn3.num_batches_tracked', 'encoder_q.net.layer2.0.conv1.weight', 'encoder_q.net.layer2.0.bn1.weight', 'encoder_q.net.layer2.0.bn1.bias', 'encoder_q.net.layer2.0.bn1.running_mean', 'encoder_q.net.layer2.0.bn1.running_var', 'encoder_q.net.layer2.0.bn1.num_batches_tracked', 'encoder_q.net.layer2.0.conv2.weight', 'encoder_q.net.layer2.0.bn2.weight', 'encoder_q.net.layer2.0.bn2.bias', 'encoder_q.net.layer2.0.bn2.running_mean', 'encoder_q.net.layer2.0.bn2.running_var', 'encoder_q.net.layer2.0.bn2.num_batches_tracked', 'encoder_q.net.layer2.0.conv3.weight', 'encoder_q.net.layer2.0.bn3.weight', 'encoder_q.net.layer2.0.bn3.bias', 'encoder_q.net.layer2.0.bn3.running_mean', 'encoder_q.net.layer2.0.bn3.running_var', 'encoder_q.net.layer2.0.bn3.num_batches_tracked', 'encoder_q.net.layer2.0.downsample.0.weight', 'encoder_q.net.layer2.0.downsample.1.weight', 'encoder_q.net.layer2.0.downsample.1.bias', 'encoder_q.net.layer2.0.downsample.1.running_mean', 'encoder_q.net.layer2.0.downsample.1.running_var', 'encoder_q.net.layer2.0.downsample.1.num_batches_tracked', 'encoder_q.net.layer2.1.conv1.weight', 'encoder_q.net.layer2.1.bn1.weight', 'encoder_q.net.layer2.1.bn1.bias', 'encoder_q.net.layer2.1.bn1.running_mean', 'encoder_q.net.layer2.1.bn1.running_var', 'encoder_q.net.layer2.1.bn1.num_batches_tracked', 'encoder_q.net.layer2.1.conv2.weight', 'encoder_q.net.layer2.1.bn2.weight', 'encoder_q.net.layer2.1.bn2.bias', 'encoder_q.net.layer2.1.bn2.running_mean', 'encoder_q.net.layer2.1.bn2.running_var', 'encoder_q.net.layer2.1.bn2.num_batches_tracked', 'encoder_q.net.layer2.1.conv3.weight', 'encoder_q.net.layer2.1.bn3.weight', 'encoder_q.net.layer2.1.bn3.bias', 'encoder_q.net.layer2.1.bn3.running_mean', 'encoder_q.net.layer2.1.bn3.running_var', 'encoder_q.net.layer2.1.bn3.num_batches_tracked', 'encoder_q.net.layer2.2.conv1.weight', 'encoder_q.net.layer2.2.bn1.weight', 'encoder_q.net.layer2.2.bn1.bias', 'encoder_q.net.layer2.2.bn1.running_mean', 'encoder_q.net.layer2.2.bn1.running_var', 'encoder_q.net.layer2.2.bn1.num_batches_tracked', 'encoder_q.net.layer2.2.conv2.weight', 'encoder_q.net.layer2.2.bn2.weight', 'encoder_q.net.layer2.2.bn2.bias', 'encoder_q.net.layer2.2.bn2.running_mean', 'encoder_q.net.layer2.2.bn2.running_var', 'encoder_q.net.layer2.2.bn2.num_batches_tracked', 'encoder_q.net.layer2.2.conv3.weight', 'encoder_q.net.layer2.2.bn3.weight', 'encoder_q.net.layer2.2.bn3.bias', 'encoder_q.net.layer2.2.bn3.running_mean', 'encoder_q.net.layer2.2.bn3.running_var', 'encoder_q.net.layer2.2.bn3.num_batches_tracked', 'encoder_q.net.layer2.3.conv1.weight', 'encoder_q.net.layer2.3.bn1.weight', 'encoder_q.net.layer2.3.bn1.bias', 'encoder_q.net.layer2.3.bn1.running_mean', 'encoder_q.net.layer2.3.bn1.running_var', 'encoder_q.net.layer2.3.bn1.num_batches_tracked', 'encoder_q.net.layer2.3.conv2.weight', 'encoder_q.net.layer2.3.bn2.weight', 'encoder_q.net.layer2.3.bn2.bias', 'encoder_q.net.layer2.3.bn2.running_mean', 'encoder_q.net.layer2.3.bn2.running_var', 'encoder_q.net.layer2.3.bn2.num_batches_tracked', 'encoder_q.net.layer2.3.conv3.weight', 'encoder_q.net.layer2.3.bn3.weight', 'encoder_q.net.layer2.3.bn3.bias', 'encoder_q.net.layer2.3.bn3.running_mean', 'encoder_q.net.layer2.3.bn3.running_var', 'encoder_q.net.layer2.3.bn3.num_batches_tracked', 'encoder_q.net.layer3.0.conv1.weight', 'encoder_q.net.layer3.0.bn1.weight', 'encoder_q.net.layer3.0.bn1.bias', 'encoder_q.net.layer3.0.bn1.running_mean', 'encoder_q.net.layer3.0.bn1.running_var', 'encoder_q.net.layer3.0.bn1.num_batches_tracked', 'encoder_q.net.layer3.0.conv2.weight', 'encoder_q.net.layer3.0.bn2.weight', 'encoder_q.net.layer3.0.bn2.bias', 'encoder_q.net.layer3.0.bn2.running_mean', 'encoder_q.net.layer3.0.bn2.running_var', 'encoder_q.net.layer3.0.bn2.num_batches_tracked', 'encoder_q.net.layer3.0.conv3.weight', 'encoder_q.net.layer3.0.bn3.weight', 'encoder_q.net.layer3.0.bn3.bias', 'encoder_q.net.layer3.0.bn3.running_mean', 'encoder_q.net.layer3.0.bn3.running_var', 'encoder_q.net.layer3.0.bn3.num_batches_tracked', 'encoder_q.net.layer3.0.downsample.0.weight', 'encoder_q.net.layer3.0.downsample.1.weight', 'encoder_q.net.layer3.0.downsample.1.bias', 'encoder_q.net.layer3.0.downsample.1.running_mean', 'encoder_q.net.layer3.0.downsample.1.running_var', 'encoder_q.net.layer3.0.downsample.1.num_batches_tracked', 'encoder_q.net.layer3.1.conv1.weight', 'encoder_q.net.layer3.1.bn1.weight', 'encoder_q.net.layer3.1.bn1.bias', 'encoder_q.net.layer3.1.bn1.running_mean', 'encoder_q.net.layer3.1.bn1.running_var', 'encoder_q.net.layer3.1.bn1.num_batches_tracked', 'encoder_q.net.layer3.1.conv2.weight', 'encoder_q.net.layer3.1.bn2.weight', 'encoder_q.net.layer3.1.bn2.bias', 'encoder_q.net.layer3.1.bn2.running_mean', 'encoder_q.net.layer3.1.bn2.running_var', 'encoder_q.net.layer3.1.bn2.num_batches_tracked', 'encoder_q.net.layer3.1.conv3.weight', 'encoder_q.net.layer3.1.bn3.weight', 'encoder_q.net.layer3.1.bn3.bias', 'encoder_q.net.layer3.1.bn3.running_mean', 'encoder_q.net.layer3.1.bn3.running_var', 'encoder_q.net.layer3.1.bn3.num_batches_tracked', 'encoder_q.net.layer3.2.conv1.weight', 'encoder_q.net.layer3.2.bn1.weight', 'encoder_q.net.layer3.2.bn1.bias', 'encoder_q.net.layer3.2.bn1.running_mean', 'encoder_q.net.layer3.2.bn1.running_var', 'encoder_q.net.layer3.2.bn1.num_batches_tracked', 'encoder_q.net.layer3.2.conv2.weight', 'encoder_q.net.layer3.2.bn2.weight', 'encoder_q.net.layer3.2.bn2.bias', 'encoder_q.net.layer3.2.bn2.running_mean', 'encoder_q.net.layer3.2.bn2.running_var', 'encoder_q.net.layer3.2.bn2.num_batches_tracked', 'encoder_q.net.layer3.2.conv3.weight', 'encoder_q.net.layer3.2.bn3.weight', 'encoder_q.net.layer3.2.bn3.bias', 'encoder_q.net.layer3.2.bn3.running_mean', 'encoder_q.net.layer3.2.bn3.running_var', 'encoder_q.net.layer3.2.bn3.num_batches_tracked', 'encoder_q.net.layer3.3.conv1.weight', 'encoder_q.net.layer3.3.bn1.weight', 'encoder_q.net.layer3.3.bn1.bias', 'encoder_q.net.layer3.3.bn1.running_mean', 'encoder_q.net.layer3.3.bn1.running_var', 'encoder_q.net.layer3.3.bn1.num_batches_tracked', 'encoder_q.net.layer3.3.conv2.weight', 'encoder_q.net.layer3.3.bn2.weight', 'encoder_q.net.layer3.3.bn2.bias', 'encoder_q.net.layer3.3.bn2.running_mean', 'encoder_q.net.layer3.3.bn2.running_var', 'encoder_q.net.layer3.3.bn2.num_batches_tracked', 'encoder_q.net.layer3.3.conv3.weight', 'encoder_q.net.layer3.3.bn3.weight', 'encoder_q.net.layer3.3.bn3.bias', 'encoder_q.net.layer3.3.bn3.running_mean', 'encoder_q.net.layer3.3.bn3.running_var', 'encoder_q.net.layer3.3.bn3.num_batches_tracked', 'encoder_q.net.layer3.4.conv1.weight', 'encoder_q.net.layer3.4.bn1.weight', 'encoder_q.net.layer3.4.bn1.bias', 'encoder_q.net.layer3.4.bn1.running_mean', 'encoder_q.net.layer3.4.bn1.running_var', 'encoder_q.net.layer3.4.bn1.num_batches_tracked', 'encoder_q.net.layer3.4.conv2.weight', 'encoder_q.net.layer3.4.bn2.weight', 'encoder_q.net.layer3.4.bn2.bias', 'encoder_q.net.layer3.4.bn2.running_mean', 'encoder_q.net.layer3.4.bn2.running_var', 'encoder_q.net.layer3.4.bn2.num_batches_tracked', 'encoder_q.net.layer3.4.conv3.weight', 'encoder_q.net.layer3.4.bn3.weight', 'encoder_q.net.layer3.4.bn3.bias', 'encoder_q.net.layer3.4.bn3.running_mean', 'encoder_q.net.layer3.4.bn3.running_var', 'encoder_q.net.layer3.4.bn3.num_batches_tracked', 'encoder_q.net.layer3.5.conv1.weight', 'encoder_q.net.layer3.5.bn1.weight', 'encoder_q.net.layer3.5.bn1.bias', 'encoder_q.net.layer3.5.bn1.running_mean', 'encoder_q.net.layer3.5.bn1.running_var', 'encoder_q.net.layer3.5.bn1.num_batches_tracked', 'encoder_q.net.layer3.5.conv2.weight', 'encoder_q.net.layer3.5.bn2.weight', 'encoder_q.net.layer3.5.bn2.bias', 'encoder_q.net.layer3.5.bn2.running_mean', 'encoder_q.net.layer3.5.bn2.running_var', 'encoder_q.net.layer3.5.bn2.num_batches_tracked', 'encoder_q.net.layer3.5.conv3.weight', 'encoder_q.net.layer3.5.bn3.weight', 'encoder_q.net.layer3.5.bn3.bias', 'encoder_q.net.layer3.5.bn3.running_mean', 'encoder_q.net.layer3.5.bn3.running_var', 'encoder_q.net.layer3.5.bn3.num_batches_tracked', 'encoder_q.net.layer4.0.conv1.weight', 'encoder_q.net.layer4.0.bn1.weight', 'encoder_q.net.layer4.0.bn1.bias', 'encoder_q.net.layer4.0.bn1.running_mean', 'encoder_q.net.layer4.0.bn1.running_var', 'encoder_q.net.layer4.0.bn1.num_batches_tracked', 'encoder_q.net.layer4.0.conv2.weight', 'encoder_q.net.layer4.0.bn2.weight', 'encoder_q.net.layer4.0.bn2.bias', 'encoder_q.net.layer4.0.bn2.running_mean', 'encoder_q.net.layer4.0.bn2.running_var', 'encoder_q.net.layer4.0.bn2.num_batches_tracked', 'encoder_q.net.layer4.0.conv3.weight', 'encoder_q.net.layer4.0.bn3.weight', 'encoder_q.net.layer4.0.bn3.bias', 'encoder_q.net.layer4.0.bn3.running_mean', 'encoder_q.net.layer4.0.bn3.running_var', 'encoder_q.net.layer4.0.bn3.num_batches_tracked', 'encoder_q.net.layer4.0.downsample.0.weight', 'encoder_q.net.layer4.0.downsample.1.weight', 'encoder_q.net.layer4.0.downsample.1.bias', 'encoder_q.net.layer4.0.downsample.1.running_mean', 'encoder_q.net.layer4.0.downsample.1.running_var', 'encoder_q.net.layer4.0.downsample.1.num_batches_tracked', 'encoder_q.net.layer4.1.conv1.weight', 'encoder_q.net.layer4.1.bn1.weight', 'encoder_q.net.layer4.1.bn1.bias', 'encoder_q.net.layer4.1.bn1.running_mean', 'encoder_q.net.layer4.1.bn1.running_var', 'encoder_q.net.layer4.1.bn1.num_batches_tracked', 'encoder_q.net.layer4.1.conv2.weight', 'encoder_q.net.layer4.1.bn2.weight', 'encoder_q.net.layer4.1.bn2.bias', 'encoder_q.net.layer4.1.bn2.running_mean', 'encoder_q.net.layer4.1.bn2.running_var', 'encoder_q.net.layer4.1.bn2.num_batches_tracked', 'encoder_q.net.layer4.1.conv3.weight', 'encoder_q.net.layer4.1.bn3.weight', 'encoder_q.net.layer4.1.bn3.bias', 'encoder_q.net.layer4.1.bn3.running_mean', 'encoder_q.net.layer4.1.bn3.running_var', 'encoder_q.net.layer4.1.bn3.num_batches_tracked', 'encoder_q.net.layer4.2.conv1.weight', 'encoder_q.net.layer4.2.bn1.weight', 'encoder_q.net.layer4.2.bn1.bias', 'encoder_q.net.layer4.2.bn1.running_mean', 'encoder_q.net.layer4.2.bn1.running_var', 'encoder_q.net.layer4.2.bn1.num_batches_tracked', 'encoder_q.net.layer4.2.conv2.weight', 'encoder_q.net.layer4.2.bn2.weight', 'encoder_q.net.layer4.2.bn2.bias', 'encoder_q.net.layer4.2.bn2.running_mean', 'encoder_q.net.layer4.2.bn2.running_var', 'encoder_q.net.layer4.2.bn2.num_batches_tracked', 'encoder_q.net.layer4.2.conv3.weight', 'encoder_q.net.layer4.2.bn3.weight', 'encoder_q.net.layer4.2.bn3.bias', 'encoder_q.net.layer4.2.bn3.running_mean', 'encoder_q.net.layer4.2.bn3.running_var', 'encoder_q.net.layer4.2.bn3.num_batches_tracked', 'encoder_k.net.conv1.weight', 'encoder_k.net.bn1.weight', 'encoder_k.net.bn1.bias', 'encoder_k.net.bn1.running_mean', 'encoder_k.net.bn1.running_var', 'encoder_k.net.bn1.num_batches_tracked', 'encoder_k.net.layer1.0.conv1.weight', 'encoder_k.net.layer1.0.bn1.weight', 'encoder_k.net.layer1.0.bn1.bias', 'encoder_k.net.layer1.0.bn1.running_mean', 'encoder_k.net.layer1.0.bn1.running_var', 'encoder_k.net.layer1.0.bn1.num_batches_tracked', 'encoder_k.net.layer1.0.conv2.weight', 'encoder_k.net.layer1.0.bn2.weight', 'encoder_k.net.layer1.0.bn2.bias', 'encoder_k.net.layer1.0.bn2.running_mean', 'encoder_k.net.layer1.0.bn2.running_var', 'encoder_k.net.layer1.0.bn2.num_batches_tracked', 'encoder_k.net.layer1.0.conv3.weight', 'encoder_k.net.layer1.0.bn3.weight', 'encoder_k.net.layer1.0.bn3.bias', 'encoder_k.net.layer1.0.bn3.running_mean', 'encoder_k.net.layer1.0.bn3.running_var', 'encoder_k.net.layer1.0.bn3.num_batches_tracked', 'encoder_k.net.layer1.0.downsample.0.weight', 'encoder_k.net.layer1.0.downsample.1.weight', 'encoder_k.net.layer1.0.downsample.1.bias', 'encoder_k.net.layer1.0.downsample.1.running_mean', 'encoder_k.net.layer1.0.downsample.1.running_var', 'encoder_k.net.layer1.0.downsample.1.num_batches_tracked', 'encoder_k.net.layer1.1.conv1.weight', 'encoder_k.net.layer1.1.bn1.weight', 'encoder_k.net.layer1.1.bn1.bias', 'encoder_k.net.layer1.1.bn1.running_mean', 'encoder_k.net.layer1.1.bn1.running_var', 'encoder_k.net.layer1.1.bn1.num_batches_tracked', 'encoder_k.net.layer1.1.conv2.weight', 'encoder_k.net.layer1.1.bn2.weight', 'encoder_k.net.layer1.1.bn2.bias', 'encoder_k.net.layer1.1.bn2.running_mean', 'encoder_k.net.layer1.1.bn2.running_var', 'encoder_k.net.layer1.1.bn2.num_batches_tracked', 'encoder_k.net.layer1.1.conv3.weight', 'encoder_k.net.layer1.1.bn3.weight', 'encoder_k.net.layer1.1.bn3.bias', 'encoder_k.net.layer1.1.bn3.running_mean', 'encoder_k.net.layer1.1.bn3.running_var', 'encoder_k.net.layer1.1.bn3.num_batches_tracked', 'encoder_k.net.layer1.2.conv1.weight', 'encoder_k.net.layer1.2.bn1.weight', 'encoder_k.net.layer1.2.bn1.bias', 'encoder_k.net.layer1.2.bn1.running_mean', 'encoder_k.net.layer1.2.bn1.running_var', 'encoder_k.net.layer1.2.bn1.num_batches_tracked', 'encoder_k.net.layer1.2.conv2.weight', 'encoder_k.net.layer1.2.bn2.weight', 'encoder_k.net.layer1.2.bn2.bias', 'encoder_k.net.layer1.2.bn2.running_mean', 'encoder_k.net.layer1.2.bn2.running_var', 'encoder_k.net.layer1.2.bn2.num_batches_tracked', 'encoder_k.net.layer1.2.conv3.weight', 'encoder_k.net.layer1.2.bn3.weight', 'encoder_k.net.layer1.2.bn3.bias', 'encoder_k.net.layer1.2.bn3.running_mean', 'encoder_k.net.layer1.2.bn3.running_var', 'encoder_k.net.layer1.2.bn3.num_batches_tracked', 'encoder_k.net.layer2.0.conv1.weight', 'encoder_k.net.layer2.0.bn1.weight', 'encoder_k.net.layer2.0.bn1.bias', 'encoder_k.net.layer2.0.bn1.running_mean', 'encoder_k.net.layer2.0.bn1.running_var', 'encoder_k.net.layer2.0.bn1.num_batches_tracked', 'encoder_k.net.layer2.0.conv2.weight', 'encoder_k.net.layer2.0.bn2.weight', 'encoder_k.net.layer2.0.bn2.bias', 'encoder_k.net.layer2.0.bn2.running_mean', 'encoder_k.net.layer2.0.bn2.running_var', 'encoder_k.net.layer2.0.bn2.num_batches_tracked', 'encoder_k.net.layer2.0.conv3.weight', 'encoder_k.net.layer2.0.bn3.weight', 'encoder_k.net.layer2.0.bn3.bias', 'encoder_k.net.layer2.0.bn3.running_mean', 'encoder_k.net.layer2.0.bn3.running_var', 'encoder_k.net.layer2.0.bn3.num_batches_tracked', 'encoder_k.net.layer2.0.downsample.0.weight', 'encoder_k.net.layer2.0.downsample.1.weight', 'encoder_k.net.layer2.0.downsample.1.bias', 'encoder_k.net.layer2.0.downsample.1.running_mean', 'encoder_k.net.layer2.0.downsample.1.running_var', 'encoder_k.net.layer2.0.downsample.1.num_batches_tracked', 'encoder_k.net.layer2.1.conv1.weight', 'encoder_k.net.layer2.1.bn1.weight', 'encoder_k.net.layer2.1.bn1.bias', 'encoder_k.net.layer2.1.bn1.running_mean', 'encoder_k.net.layer2.1.bn1.running_var', 'encoder_k.net.layer2.1.bn1.num_batches_tracked', 'encoder_k.net.layer2.1.conv2.weight', 'encoder_k.net.layer2.1.bn2.weight', 'encoder_k.net.layer2.1.bn2.bias', 'encoder_k.net.layer2.1.bn2.running_mean', 'encoder_k.net.layer2.1.bn2.running_var', 'encoder_k.net.layer2.1.bn2.num_batches_tracked', 'encoder_k.net.layer2.1.conv3.weight', 'encoder_k.net.layer2.1.bn3.weight', 'encoder_k.net.layer2.1.bn3.bias', 'encoder_k.net.layer2.1.bn3.running_mean', 'encoder_k.net.layer2.1.bn3.running_var', 'encoder_k.net.layer2.1.bn3.num_batches_tracked', 'encoder_k.net.layer2.2.conv1.weight', 'encoder_k.net.layer2.2.bn1.weight', 'encoder_k.net.layer2.2.bn1.bias', 'encoder_k.net.layer2.2.bn1.running_mean', 'encoder_k.net.layer2.2.bn1.running_var', 'encoder_k.net.layer2.2.bn1.num_batches_tracked', 'encoder_k.net.layer2.2.conv2.weight', 'encoder_k.net.layer2.2.bn2.weight', 'encoder_k.net.layer2.2.bn2.bias', 'encoder_k.net.layer2.2.bn2.running_mean', 'encoder_k.net.layer2.2.bn2.running_var', 'encoder_k.net.layer2.2.bn2.num_batches_tracked', 'encoder_k.net.layer2.2.conv3.weight', 'encoder_k.net.layer2.2.bn3.weight', 'encoder_k.net.layer2.2.bn3.bias', 'encoder_k.net.layer2.2.bn3.running_mean', 'encoder_k.net.layer2.2.bn3.running_var', 'encoder_k.net.layer2.2.bn3.num_batches_tracked', 'encoder_k.net.layer2.3.conv1.weight', 'encoder_k.net.layer2.3.bn1.weight', 'encoder_k.net.layer2.3.bn1.bias', 'encoder_k.net.layer2.3.bn1.running_mean', 'encoder_k.net.layer2.3.bn1.running_var', 'encoder_k.net.layer2.3.bn1.num_batches_tracked', 'encoder_k.net.layer2.3.conv2.weight', 'encoder_k.net.layer2.3.bn2.weight', 'encoder_k.net.layer2.3.bn2.bias', 'encoder_k.net.layer2.3.bn2.running_mean', 'encoder_k.net.layer2.3.bn2.running_var', 'encoder_k.net.layer2.3.bn2.num_batches_tracked', 'encoder_k.net.layer2.3.conv3.weight', 'encoder_k.net.layer2.3.bn3.weight', 'encoder_k.net.layer2.3.bn3.bias', 'encoder_k.net.layer2.3.bn3.running_mean', 'encoder_k.net.layer2.3.bn3.running_var', 'encoder_k.net.layer2.3.bn3.num_batches_tracked', 'encoder_k.net.layer3.0.conv1.weight', 'encoder_k.net.layer3.0.bn1.weight', 'encoder_k.net.layer3.0.bn1.bias', 'encoder_k.net.layer3.0.bn1.running_mean', 'encoder_k.net.layer3.0.bn1.running_var', 'encoder_k.net.layer3.0.bn1.num_batches_tracked', 'encoder_k.net.layer3.0.conv2.weight', 'encoder_k.net.layer3.0.bn2.weight', 'encoder_k.net.layer3.0.bn2.bias', 'encoder_k.net.layer3.0.bn2.running_mean', 'encoder_k.net.layer3.0.bn2.running_var', 'encoder_k.net.layer3.0.bn2.num_batches_tracked', 'encoder_k.net.layer3.0.conv3.weight', 'encoder_k.net.layer3.0.bn3.weight', 'encoder_k.net.layer3.0.bn3.bias', 'encoder_k.net.layer3.0.bn3.running_mean', 'encoder_k.net.layer3.0.bn3.running_var', 'encoder_k.net.layer3.0.bn3.num_batches_tracked', 'encoder_k.net.layer3.0.downsample.0.weight', 'encoder_k.net.layer3.0.downsample.1.weight', 'encoder_k.net.layer3.0.downsample.1.bias', 'encoder_k.net.layer3.0.downsample.1.running_mean', 'encoder_k.net.layer3.0.downsample.1.running_var', 'encoder_k.net.layer3.0.downsample.1.num_batches_tracked', 'encoder_k.net.layer3.1.conv1.weight', 'encoder_k.net.layer3.1.bn1.weight', 'encoder_k.net.layer3.1.bn1.bias', 'encoder_k.net.layer3.1.bn1.running_mean', 'encoder_k.net.layer3.1.bn1.running_var', 'encoder_k.net.layer3.1.bn1.num_batches_tracked', 'encoder_k.net.layer3.1.conv2.weight', 'encoder_k.net.layer3.1.bn2.weight', 'encoder_k.net.layer3.1.bn2.bias', 'encoder_k.net.layer3.1.bn2.running_mean', 'encoder_k.net.layer3.1.bn2.running_var', 'encoder_k.net.layer3.1.bn2.num_batches_tracked', 'encoder_k.net.layer3.1.conv3.weight', 'encoder_k.net.layer3.1.bn3.weight', 'encoder_k.net.layer3.1.bn3.bias', 'encoder_k.net.layer3.1.bn3.running_mean', 'encoder_k.net.layer3.1.bn3.running_var', 'encoder_k.net.layer3.1.bn3.num_batches_tracked', 'encoder_k.net.layer3.2.conv1.weight', 'encoder_k.net.layer3.2.bn1.weight', 'encoder_k.net.layer3.2.bn1.bias', 'encoder_k.net.layer3.2.bn1.running_mean', 'encoder_k.net.layer3.2.bn1.running_var', 'encoder_k.net.layer3.2.bn1.num_batches_tracked', 'encoder_k.net.layer3.2.conv2.weight', 'encoder_k.net.layer3.2.bn2.weight', 'encoder_k.net.layer3.2.bn2.bias', 'encoder_k.net.layer3.2.bn2.running_mean', 'encoder_k.net.layer3.2.bn2.running_var', 'encoder_k.net.layer3.2.bn2.num_batches_tracked', 'encoder_k.net.layer3.2.conv3.weight', 'encoder_k.net.layer3.2.bn3.weight', 'encoder_k.net.layer3.2.bn3.bias', 'encoder_k.net.layer3.2.bn3.running_mean', 'encoder_k.net.layer3.2.bn3.running_var', 'encoder_k.net.layer3.2.bn3.num_batches_tracked', 'encoder_k.net.layer3.3.conv1.weight', 'encoder_k.net.layer3.3.bn1.weight', 'encoder_k.net.layer3.3.bn1.bias', 'encoder_k.net.layer3.3.bn1.running_mean', 'encoder_k.net.layer3.3.bn1.running_var', 'encoder_k.net.layer3.3.bn1.num_batches_tracked', 'encoder_k.net.layer3.3.conv2.weight', 'encoder_k.net.layer3.3.bn2.weight', 'encoder_k.net.layer3.3.bn2.bias', 'encoder_k.net.layer3.3.bn2.running_mean', 'encoder_k.net.layer3.3.bn2.running_var', 'encoder_k.net.layer3.3.bn2.num_batches_tracked', 'encoder_k.net.layer3.3.conv3.weight', 'encoder_k.net.layer3.3.bn3.weight', 'encoder_k.net.layer3.3.bn3.bias', 'encoder_k.net.layer3.3.bn3.running_mean', 'encoder_k.net.layer3.3.bn3.running_var', 'encoder_k.net.layer3.3.bn3.num_batches_tracked', 'encoder_k.net.layer3.4.conv1.weight', 'encoder_k.net.layer3.4.bn1.weight', 'encoder_k.net.layer3.4.bn1.bias', 'encoder_k.net.layer3.4.bn1.running_mean', 'encoder_k.net.layer3.4.bn1.running_var', 'encoder_k.net.layer3.4.bn1.num_batches_tracked', 'encoder_k.net.layer3.4.conv2.weight', 'encoder_k.net.layer3.4.bn2.weight', 'encoder_k.net.layer3.4.bn2.bias', 'encoder_k.net.layer3.4.bn2.running_mean', 'encoder_k.net.layer3.4.bn2.running_var', 'encoder_k.net.layer3.4.bn2.num_batches_tracked', 'encoder_k.net.layer3.4.conv3.weight', 'encoder_k.net.layer3.4.bn3.weight', 'encoder_k.net.layer3.4.bn3.bias', 'encoder_k.net.layer3.4.bn3.running_mean', 'encoder_k.net.layer3.4.bn3.running_var', 'encoder_k.net.layer3.4.bn3.num_batches_tracked', 'encoder_k.net.layer3.5.conv1.weight', 'encoder_k.net.layer3.5.bn1.weight', 'encoder_k.net.layer3.5.bn1.bias', 'encoder_k.net.layer3.5.bn1.running_mean', 'encoder_k.net.layer3.5.bn1.running_var', 'encoder_k.net.layer3.5.bn1.num_batches_tracked', 'encoder_k.net.layer3.5.conv2.weight', 'encoder_k.net.layer3.5.bn2.weight', 'encoder_k.net.layer3.5.bn2.bias', 'encoder_k.net.layer3.5.bn2.running_mean', 'encoder_k.net.layer3.5.bn2.running_var', 'encoder_k.net.layer3.5.bn2.num_batches_tracked', 'encoder_k.net.layer3.5.conv3.weight', 'encoder_k.net.layer3.5.bn3.weight', 'encoder_k.net.layer3.5.bn3.bias', 'encoder_k.net.layer3.5.bn3.running_mean', 'encoder_k.net.layer3.5.bn3.running_var', 'encoder_k.net.layer3.5.bn3.num_batches_tracked', 'encoder_k.net.layer4.0.conv1.weight', 'encoder_k.net.layer4.0.bn1.weight', 'encoder_k.net.layer4.0.bn1.bias', 'encoder_k.net.layer4.0.bn1.running_mean', 'encoder_k.net.layer4.0.bn1.running_var', 'encoder_k.net.layer4.0.bn1.num_batches_tracked', 'encoder_k.net.layer4.0.conv2.weight', 'encoder_k.net.layer4.0.bn2.weight', 'encoder_k.net.layer4.0.bn2.bias', 'encoder_k.net.layer4.0.bn2.running_mean', 'encoder_k.net.layer4.0.bn2.running_var', 'encoder_k.net.layer4.0.bn2.num_batches_tracked', 'encoder_k.net.layer4.0.conv3.weight', 'encoder_k.net.layer4.0.bn3.weight', 'encoder_k.net.layer4.0.bn3.bias', 'encoder_k.net.layer4.0.bn3.running_mean', 'encoder_k.net.layer4.0.bn3.running_var', 'encoder_k.net.layer4.0.bn3.num_batches_tracked', 'encoder_k.net.layer4.0.downsample.0.weight', 'encoder_k.net.layer4.0.downsample.1.weight', 'encoder_k.net.layer4.0.downsample.1.bias', 'encoder_k.net.layer4.0.downsample.1.running_mean', 'encoder_k.net.layer4.0.downsample.1.running_var', 'encoder_k.net.layer4.0.downsample.1.num_batches_tracked', 'encoder_k.net.layer4.1.conv1.weight', 'encoder_k.net.layer4.1.bn1.weight', 'encoder_k.net.layer4.1.bn1.bias', 'encoder_k.net.layer4.1.bn1.running_mean', 'encoder_k.net.layer4.1.bn1.running_var', 'encoder_k.net.layer4.1.bn1.num_batches_tracked', 'encoder_k.net.layer4.1.conv2.weight', 'encoder_k.net.layer4.1.bn2.weight', 'encoder_k.net.layer4.1.bn2.bias', 'encoder_k.net.layer4.1.bn2.running_mean', 'encoder_k.net.layer4.1.bn2.running_var', 'encoder_k.net.layer4.1.bn2.num_batches_tracked', 'encoder_k.net.layer4.1.conv3.weight', 'encoder_k.net.layer4.1.bn3.weight', 'encoder_k.net.layer4.1.bn3.bias', 'encoder_k.net.layer4.1.bn3.running_mean', 'encoder_k.net.layer4.1.bn3.running_var', 'encoder_k.net.layer4.1.bn3.num_batches_tracked', 'encoder_k.net.layer4.2.conv1.weight', 'encoder_k.net.layer4.2.bn1.weight', 'encoder_k.net.layer4.2.bn1.bias', 'encoder_k.net.layer4.2.bn1.running_mean', 'encoder_k.net.layer4.2.bn1.running_var', 'encoder_k.net.layer4.2.bn1.num_batches_tracked', 'encoder_k.net.layer4.2.conv2.weight', 'encoder_k.net.layer4.2.bn2.weight', 'encoder_k.net.layer4.2.bn2.bias', 'encoder_k.net.layer4.2.bn2.running_mean', 'encoder_k.net.layer4.2.bn2.running_var', 'encoder_k.net.layer4.2.bn2.num_batches_tracked', 'encoder_k.net.layer4.2.conv3.weight', 'encoder_k.net.layer4.2.bn3.weight', 'encoder_k.net.layer4.2.bn3.bias', 'encoder_k.net.layer4.2.bn3.running_mean', 'encoder_k.net.layer4.2.bn3.running_var', 'encoder_k.net.layer4.2.bn3.num_batches_tracked'])\n","=> loaded pre-trained model '/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/MoCo/checkpoint_0199.pth.tar'\n","Epoch: [0][0/4]\tTime  0.433 ( 0.433)\tData  0.168 ( 0.168)\tLoss 5.3021e+00 (5.3021e+00)\tAcc@1   0.37 (  0.37)\tAcc@5   0.37 (  0.37)\n","Test: [ 0/18]\tTime  0.566 ( 0.566)\tLoss 9.4835e+06 (9.4835e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.209 ( 0.232)\tLoss 9.7435e+06 (9.2688e+06)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.1344 Recall 0.3667 F1 Score 0.1967\n","=> loading '/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/MoCo/checkpoint_0199.pth.tar' for sanity check\n","Key not found in pre-trained model: encoder_q.net.encoder.net.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.downsample.0.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.downsample.1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.downsample.1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.downsample.1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.downsample.1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.0.downsample.1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.1.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer1.2.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.downsample.0.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.downsample.1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.downsample.1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.downsample.1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.downsample.1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.0.downsample.1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.1.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.2.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer2.3.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.downsample.0.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.downsample.1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.downsample.1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.downsample.1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.downsample.1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.0.downsample.1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.1.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.2.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.3.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.4.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer3.5.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.downsample.0.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.downsample.1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.downsample.1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.downsample.1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.downsample.1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.0.downsample.1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.1.bn3.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.conv1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn1.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn1.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn1.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn1.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn1.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.conv2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn2.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn2.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn2.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn2.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn2.num_batches_tracked\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.conv3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn3.weight\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn3.bias\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn3.running_mean\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn3.running_var\n","Key not found in pre-trained model: encoder_q.net.encoder.net.layer4.2.bn3.num_batches_tracked\n","=> sanity check passed.\n","Epoch: [1][0/4]\tTime  0.605 ( 0.605)\tData  0.358 ( 0.358)\tLoss 8.6551e+06 (8.6551e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [ 0/18]\tTime  0.503 ( 0.503)\tLoss 1.4379e+07 (1.4379e+07)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.128 ( 0.202)\tLoss 1.5556e+07 (1.5904e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [2][0/4]\tTime  0.225 ( 0.225)\tData  0.127 ( 0.127)\tLoss 1.2220e+07 (1.2220e+07)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.329 ( 0.329)\tLoss 1.9736e+07 (1.9736e+07)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.122 ( 0.166)\tLoss 2.0981e+07 (2.0886e+07)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.1344 Recall 0.3667 F1 Score 0.1967\n","Epoch: [3][0/4]\tTime  0.402 ( 0.402)\tData  0.231 ( 0.231)\tLoss 2.2079e+07 (2.2079e+07)\tAcc@1   0.27 (  0.27)\tAcc@5   0.27 (  0.27)\n","Test: [ 0/18]\tTime  0.337 ( 0.337)\tLoss 2.5240e+07 (2.5240e+07)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.085 ( 0.113)\tLoss 2.6994e+07 (2.7505e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [4][0/4]\tTime  0.221 ( 0.221)\tData  0.129 ( 0.129)\tLoss 2.0671e+07 (2.0671e+07)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.238 ( 0.238)\tLoss 8.5167e+06 (8.5167e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.105 ( 0.108)\tLoss 1.0698e+07 (1.0454e+07)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.1344 Recall 0.3667 F1 Score 0.1967\n","Epoch: [5][0/4]\tTime  0.222 ( 0.222)\tData  0.114 ( 0.114)\tLoss 9.8660e+06 (9.8660e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [ 0/18]\tTime  0.227 ( 0.227)\tLoss 1.1144e+07 (1.1144e+07)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.085 ( 0.107)\tLoss 1.6606e+07 (1.5875e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [6][0/4]\tTime  0.226 ( 0.226)\tData  0.126 ( 0.126)\tLoss 1.4449e+07 (1.4449e+07)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.206 ( 0.206)\tLoss 1.2141e+07 (1.2141e+07)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.086 ( 0.097)\tLoss 1.7264e+07 (1.6889e+07)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.1344 Recall 0.3667 F1 Score 0.1967\n","Epoch: [7][0/4]\tTime  0.207 ( 0.207)\tData  0.117 ( 0.117)\tLoss 1.5990e+07 (1.5990e+07)\tAcc@1   0.38 (  0.38)\tAcc@5   0.38 (  0.38)\n","Test: [ 0/18]\tTime  0.276 ( 0.276)\tLoss 6.9098e+06 (6.9098e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.196 ( 0.153)\tLoss 1.4116e+07 (1.2643e+07)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.1344 Recall 0.3667 F1 Score 0.1967\n","Epoch: [8][0/4]\tTime  0.335 ( 0.335)\tData  0.192 ( 0.192)\tLoss 1.3269e+07 (1.3269e+07)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [ 0/18]\tTime  0.300 ( 0.300)\tLoss 1.2288e+07 (1.2288e+07)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.084 ( 0.105)\tLoss 1.8882e+07 (1.7549e+07)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.1344 Recall 0.3667 F1 Score 0.1967\n","Epoch: [9][0/4]\tTime  0.217 ( 0.217)\tData  0.121 ( 0.121)\tLoss 1.7209e+07 (1.7209e+07)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n","Test: [ 0/18]\tTime  0.208 ( 0.208)\tLoss 3.6860e+06 (3.6860e+06)\tAcc@1   0.31 (  0.31)\tAcc@5   0.31 (  0.31)\n","Test: [10/18]\tTime  0.085 ( 0.097)\tLoss 8.4933e+06 (7.2707e+06)\tAcc@1   0.29 (  0.32)\tAcc@5   0.29 (  0.32)\n"," * Acc@1 0.3196 Acc@5 0.3196 Precision 0.3294 Recall 0.3196 F1 Score 0.3122\n","Epoch: [10][0/4]\tTime  0.229 ( 0.229)\tData  0.133 ( 0.133)\tLoss 3.5910e+06 (3.5910e+06)\tAcc@1   0.58 (  0.58)\tAcc@5   0.58 (  0.58)\n","Test: [ 0/18]\tTime  0.203 ( 0.203)\tLoss 6.7244e+06 (6.7244e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.085 ( 0.097)\tLoss 1.2042e+07 (1.0456e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [11][0/4]\tTime  0.228 ( 0.228)\tData  0.131 ( 0.131)\tLoss 8.5259e+06 (8.5259e+06)\tAcc@1   0.65 (  0.65)\tAcc@5   0.65 (  0.65)\n","Test: [ 0/18]\tTime  0.227 ( 0.227)\tLoss 7.5674e+06 (7.5674e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.091 ( 0.100)\tLoss 1.2493e+07 (1.1671e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [12][0/4]\tTime  0.211 ( 0.211)\tData  0.118 ( 0.118)\tLoss 6.1756e+06 (6.1756e+06)\tAcc@1   0.64 (  0.64)\tAcc@5   0.64 (  0.64)\n","Test: [ 0/18]\tTime  0.205 ( 0.205)\tLoss 5.9727e+06 (5.9727e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.145 ( 0.132)\tLoss 1.1636e+07 (9.6828e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [13][0/4]\tTime  0.354 ( 0.354)\tData  0.176 ( 0.176)\tLoss 1.1716e+07 (1.1716e+07)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [ 0/18]\tTime  0.299 ( 0.299)\tLoss 7.0285e+06 (7.0285e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.084 ( 0.136)\tLoss 1.1095e+07 (9.9025e+06)\tAcc@1   0.37 (  0.37)\tAcc@5   0.37 (  0.37)\n"," * Acc@1 0.3693 Acc@5 0.3693 Precision 0.4797 Recall 0.3693 F1 Score 0.2049\n","Epoch: [14][0/4]\tTime  0.223 ( 0.223)\tData  0.117 ( 0.117)\tLoss 1.2589e+07 (1.2589e+07)\tAcc@1   0.40 (  0.40)\tAcc@5   0.40 (  0.40)\n","Test: [ 0/18]\tTime  0.200 ( 0.200)\tLoss 4.9704e+06 (4.9704e+06)\tAcc@1   0.07 (  0.07)\tAcc@5   0.07 (  0.07)\n","Test: [10/18]\tTime  0.088 ( 0.098)\tLoss 8.0383e+06 (7.1439e+06)\tAcc@1   0.05 (  0.07)\tAcc@5   0.05 (  0.07)\n"," * Acc@1 0.0689 Acc@5 0.0689 Precision 0.2739 Recall 0.0689 F1 Score 0.0745\n","Epoch: [15][0/4]\tTime  0.224 ( 0.224)\tData  0.126 ( 0.126)\tLoss 4.5300e+06 (4.5300e+06)\tAcc@1   0.27 (  0.27)\tAcc@5   0.27 (  0.27)\n","Test: [ 0/18]\tTime  0.203 ( 0.203)\tLoss 4.6555e+06 (4.6555e+06)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n","Test: [10/18]\tTime  0.083 ( 0.098)\tLoss 8.2436e+06 (7.1824e+06)\tAcc@1   0.38 (  0.37)\tAcc@5   0.38 (  0.37)\n"," * Acc@1 0.3756 Acc@5 0.3756 Precision 0.4530 Recall 0.3756 F1 Score 0.2316\n","Epoch: [16][0/4]\tTime  0.215 ( 0.215)\tData  0.119 ( 0.119)\tLoss 5.2793e+06 (5.2793e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [ 0/18]\tTime  0.209 ( 0.209)\tLoss 9.9983e+06 (9.9983e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.100 ( 0.098)\tLoss 1.2510e+07 (1.1784e+07)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.1344 Recall 0.3667 F1 Score 0.1967\n","Epoch: [17][0/4]\tTime  0.244 ( 0.244)\tData  0.132 ( 0.132)\tLoss 1.0244e+07 (1.0244e+07)\tAcc@1   0.38 (  0.38)\tAcc@5   0.38 (  0.38)\n","Test: [ 0/18]\tTime  0.224 ( 0.224)\tLoss 1.0128e+07 (1.0128e+07)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.148 ( 0.121)\tLoss 1.2662e+07 (1.2076e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [18][0/4]\tTime  0.375 ( 0.375)\tData  0.190 ( 0.190)\tLoss 8.6579e+06 (8.6579e+06)\tAcc@1   0.50 (  0.50)\tAcc@5   0.50 (  0.50)\n","Test: [ 0/18]\tTime  0.311 ( 0.311)\tLoss 5.6398e+06 (5.6398e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.082 ( 0.154)\tLoss 8.6755e+06 (7.9189e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [19][0/4]\tTime  0.231 ( 0.231)\tData  0.129 ( 0.129)\tLoss 5.6765e+06 (5.6765e+06)\tAcc@1   0.58 (  0.58)\tAcc@5   0.58 (  0.58)\n","Test: [ 0/18]\tTime  0.211 ( 0.211)\tLoss 4.3168e+06 (4.3168e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.088 ( 0.097)\tLoss 7.6460e+06 (7.2515e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [20][0/4]\tTime  0.237 ( 0.237)\tData  0.138 ( 0.138)\tLoss 6.7977e+06 (6.7977e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [ 0/18]\tTime  0.207 ( 0.207)\tLoss 8.1162e+06 (8.1162e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.096 ( 0.103)\tLoss 1.1551e+07 (1.0973e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [21][0/4]\tTime  0.226 ( 0.226)\tData  0.127 ( 0.127)\tLoss 8.5586e+06 (8.5586e+06)\tAcc@1   0.55 (  0.55)\tAcc@5   0.55 (  0.55)\n","Test: [ 0/18]\tTime  0.213 ( 0.213)\tLoss 2.9160e+06 (2.9160e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.086 ( 0.100)\tLoss 5.3992e+06 (5.1753e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [22][0/4]\tTime  0.229 ( 0.229)\tData  0.134 ( 0.134)\tLoss 5.7962e+06 (5.7962e+06)\tAcc@1   0.60 (  0.60)\tAcc@5   0.60 (  0.60)\n","Test: [ 0/18]\tTime  0.211 ( 0.211)\tLoss 1.3440e+07 (1.3440e+07)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.092 ( 0.099)\tLoss 1.5150e+07 (1.4931e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [23][0/4]\tTime  0.368 ( 0.368)\tData  0.206 ( 0.206)\tLoss 9.9673e+06 (9.9673e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [ 0/18]\tTime  0.360 ( 0.360)\tLoss 7.5596e+06 (7.5596e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.154 ( 0.167)\tLoss 8.6504e+06 (8.1643e+06)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.1344 Recall 0.3667 F1 Score 0.1967\n","Epoch: [24][0/4]\tTime  0.230 ( 0.230)\tData  0.128 ( 0.128)\tLoss 4.7019e+06 (4.7019e+06)\tAcc@1   0.39 (  0.39)\tAcc@5   0.39 (  0.39)\n","Test: [ 0/18]\tTime  0.219 ( 0.219)\tLoss 5.2849e+06 (5.2849e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.085 ( 0.100)\tLoss 6.8520e+06 (6.2015e+06)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3693 Acc@5 0.3693 Precision 0.6274 Recall 0.3693 F1 Score 0.2025\n","Epoch: [25][0/4]\tTime  0.245 ( 0.245)\tData  0.130 ( 0.130)\tLoss 3.5797e+06 (3.5797e+06)\tAcc@1   0.41 (  0.41)\tAcc@5   0.41 (  0.41)\n","Test: [ 0/18]\tTime  0.200 ( 0.200)\tLoss 3.4231e+06 (3.4231e+06)\tAcc@1   0.32 (  0.32)\tAcc@5   0.32 (  0.32)\n","Test: [10/18]\tTime  0.088 ( 0.097)\tLoss 4.9372e+06 (4.3584e+06)\tAcc@1   0.25 (  0.27)\tAcc@5   0.25 (  0.27)\n"," * Acc@1 0.2751 Acc@5 0.2751 Precision 0.4296 Recall 0.2751 F1 Score 0.1826\n","Epoch: [26][0/4]\tTime  0.244 ( 0.244)\tData  0.137 ( 0.137)\tLoss 1.4779e+06 (1.4779e+06)\tAcc@1   0.25 (  0.25)\tAcc@5   0.25 (  0.25)\n","Test: [ 0/18]\tTime  0.201 ( 0.201)\tLoss 1.6197e+06 (1.6197e+06)\tAcc@1   0.38 (  0.38)\tAcc@5   0.38 (  0.38)\n","Test: [10/18]\tTime  0.084 ( 0.099)\tLoss 3.6358e+06 (3.3079e+06)\tAcc@1   0.38 (  0.37)\tAcc@5   0.38 (  0.37)\n"," * Acc@1 0.3613 Acc@5 0.3613 Precision 0.3754 Recall 0.3613 F1 Score 0.3392\n","Epoch: [27][0/4]\tTime  0.205 ( 0.205)\tData  0.111 ( 0.111)\tLoss 1.4585e+06 (1.4585e+06)\tAcc@1   0.58 (  0.58)\tAcc@5   0.58 (  0.58)\n","Test: [ 0/18]\tTime  0.203 ( 0.203)\tLoss 2.3393e+06 (2.3393e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.085 ( 0.102)\tLoss 4.7113e+06 (4.0960e+06)\tAcc@1   0.39 (  0.38)\tAcc@5   0.39 (  0.38)\n"," * Acc@1 0.3876 Acc@5 0.3876 Precision 0.4446 Recall 0.3876 F1 Score 0.2859\n","Epoch: [28][0/4]\tTime  0.338 ( 0.338)\tData  0.184 ( 0.184)\tLoss 4.2051e+06 (4.2051e+06)\tAcc@1   0.67 (  0.67)\tAcc@5   0.67 (  0.67)\n","Test: [ 0/18]\tTime  0.338 ( 0.338)\tLoss 1.4186e+06 (1.4186e+06)\tAcc@1   0.64 (  0.64)\tAcc@5   0.64 (  0.64)\n","Test: [10/18]\tTime  0.087 ( 0.244)\tLoss 3.5444e+06 (3.1012e+06)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n"," * Acc@1 0.5813 Acc@5 0.5813 Precision 0.6991 Recall 0.5813 F1 Score 0.4343\n","Epoch: [29][0/4]\tTime  0.241 ( 0.241)\tData  0.149 ( 0.149)\tLoss 2.7167e+06 (2.7167e+06)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.212 ( 0.212)\tLoss 1.3004e+06 (1.3004e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.086 ( 0.099)\tLoss 2.9143e+06 (2.5540e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [30][0/4]\tTime  0.207 ( 0.207)\tData  0.114 ( 0.114)\tLoss 3.0398e+06 (3.0398e+06)\tAcc@1   0.60 (  0.60)\tAcc@5   0.60 (  0.60)\n","Test: [ 0/18]\tTime  0.204 ( 0.204)\tLoss 4.3834e+06 (4.3834e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.084 ( 0.100)\tLoss 6.0299e+06 (5.6446e+06)\tAcc@1   0.37 (  0.36)\tAcc@5   0.37 (  0.36)\n"," * Acc@1 0.3729 Acc@5 0.3729 Precision 0.4877 Recall 0.3729 F1 Score 0.2140\n","Epoch: [31][0/4]\tTime  0.231 ( 0.231)\tData  0.132 ( 0.132)\tLoss 3.5410e+06 (3.5410e+06)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [ 0/18]\tTime  0.214 ( 0.214)\tLoss 3.2231e+06 (3.2231e+06)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n","Test: [10/18]\tTime  0.090 ( 0.102)\tLoss 4.8861e+06 (5.0805e+06)\tAcc@1   0.40 (  0.38)\tAcc@5   0.40 (  0.38)\n"," * Acc@1 0.3867 Acc@5 0.3867 Precision 0.5094 Recall 0.3867 F1 Score 0.2552\n","Epoch: [32][0/4]\tTime  0.218 ( 0.218)\tData  0.119 ( 0.119)\tLoss 4.4572e+06 (4.4572e+06)\tAcc@1   0.51 (  0.51)\tAcc@5   0.51 (  0.51)\n","Test: [ 0/18]\tTime  0.221 ( 0.221)\tLoss 4.3391e+06 (4.3391e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.149 ( 0.128)\tLoss 6.5000e+06 (6.5466e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [33][0/4]\tTime  0.360 ( 0.360)\tData  0.199 ( 0.199)\tLoss 5.1927e+06 (5.1927e+06)\tAcc@1   0.56 (  0.56)\tAcc@5   0.56 (  0.56)\n","Test: [ 0/18]\tTime  0.358 ( 0.358)\tLoss 2.6801e+06 (2.6801e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.086 ( 0.145)\tLoss 6.2009e+06 (5.7569e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [34][0/4]\tTime  0.217 ( 0.217)\tData  0.126 ( 0.126)\tLoss 4.4762e+06 (4.4762e+06)\tAcc@1   0.57 (  0.57)\tAcc@5   0.57 (  0.57)\n","Test: [ 0/18]\tTime  0.218 ( 0.218)\tLoss 8.0754e+06 (8.0754e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.105 ( 0.101)\tLoss 1.1709e+07 (1.1831e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [35][0/4]\tTime  0.218 ( 0.218)\tData  0.120 ( 0.120)\tLoss 7.0876e+06 (7.0876e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [ 0/18]\tTime  0.214 ( 0.214)\tLoss 8.1102e+06 (8.1102e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.092 ( 0.100)\tLoss 1.3008e+07 (1.1987e+07)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3671 Acc@5 0.3671 Precision 0.7092 Recall 0.3671 F1 Score 0.1977\n","Epoch: [36][0/4]\tTime  0.215 ( 0.215)\tData  0.121 ( 0.121)\tLoss 1.1268e+07 (1.1268e+07)\tAcc@1   0.37 (  0.37)\tAcc@5   0.37 (  0.37)\n","Test: [ 0/18]\tTime  0.227 ( 0.227)\tLoss 3.1904e+06 (3.1904e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.088 ( 0.100)\tLoss 7.0640e+06 (5.7253e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [37][0/4]\tTime  0.219 ( 0.219)\tData  0.125 ( 0.125)\tLoss 3.8723e+06 (3.8723e+06)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.209 ( 0.209)\tLoss 1.2208e+07 (1.2208e+07)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.123 ( 0.109)\tLoss 1.5414e+07 (1.4632e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [38][0/4]\tTime  0.362 ( 0.362)\tData  0.205 ( 0.205)\tLoss 1.3617e+07 (1.3617e+07)\tAcc@1   0.54 (  0.54)\tAcc@5   0.54 (  0.54)\n","Test: [ 0/18]\tTime  0.355 ( 0.355)\tLoss 2.2229e+07 (2.2229e+07)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.126 ( 0.160)\tLoss 2.5938e+07 (2.5151e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [39][0/4]\tTime  0.226 ( 0.226)\tData  0.132 ( 0.132)\tLoss 1.7313e+07 (1.7313e+07)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.270 ( 0.270)\tLoss 1.5133e+07 (1.5133e+07)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.089 ( 0.104)\tLoss 1.8058e+07 (1.7144e+07)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.1344 Recall 0.3667 F1 Score 0.1967\n","Epoch: [40][0/4]\tTime  0.218 ( 0.218)\tData  0.125 ( 0.125)\tLoss 9.7434e+06 (9.7434e+06)\tAcc@1   0.38 (  0.38)\tAcc@5   0.38 (  0.38)\n","Test: [ 0/18]\tTime  0.223 ( 0.223)\tLoss 3.3990e+06 (3.3990e+06)\tAcc@1   0.39 (  0.39)\tAcc@5   0.39 (  0.39)\n","Test: [10/18]\tTime  0.087 ( 0.099)\tLoss 7.2200e+06 (6.1616e+06)\tAcc@1   0.38 (  0.38)\tAcc@5   0.38 (  0.38)\n"," * Acc@1 0.3862 Acc@5 0.3862 Precision 0.5015 Recall 0.3862 F1 Score 0.2513\n","Epoch: [41][0/4]\tTime  0.231 ( 0.231)\tData  0.129 ( 0.129)\tLoss 3.3601e+06 (3.3601e+06)\tAcc@1   0.66 (  0.66)\tAcc@5   0.66 (  0.66)\n","Test: [ 0/18]\tTime  0.201 ( 0.201)\tLoss 4.5208e+06 (4.5208e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.085 ( 0.098)\tLoss 6.3836e+06 (5.9872e+06)\tAcc@1   0.38 (  0.37)\tAcc@5   0.38 (  0.37)\n"," * Acc@1 0.3729 Acc@5 0.3729 Precision 0.4898 Recall 0.3729 F1 Score 0.2244\n","Epoch: [42][0/4]\tTime  0.224 ( 0.224)\tData  0.131 ( 0.131)\tLoss 3.1512e+06 (3.1512e+06)\tAcc@1   0.45 (  0.45)\tAcc@5   0.45 (  0.45)\n","Test: [ 0/18]\tTime  0.221 ( 0.221)\tLoss 4.1640e+06 (4.1640e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.126 ( 0.105)\tLoss 7.8931e+06 (6.7743e+06)\tAcc@1   0.36 (  0.37)\tAcc@5   0.36 (  0.37)\n"," * Acc@1 0.3769 Acc@5 0.3769 Precision 0.4756 Recall 0.3769 F1 Score 0.2321\n","Epoch: [43][0/4]\tTime  0.347 ( 0.347)\tData  0.223 ( 0.223)\tLoss 3.5345e+06 (3.5345e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [ 0/18]\tTime  0.372 ( 0.372)\tLoss 2.7091e+06 (2.7091e+06)\tAcc@1   0.38 (  0.38)\tAcc@5   0.38 (  0.38)\n","Test: [10/18]\tTime  0.136 ( 0.154)\tLoss 5.3596e+06 (5.5741e+06)\tAcc@1   0.38 (  0.39)\tAcc@5   0.38 (  0.39)\n"," * Acc@1 0.3996 Acc@5 0.3996 Precision 0.4457 Recall 0.3996 F1 Score 0.3339\n","Epoch: [44][0/4]\tTime  0.214 ( 0.214)\tData  0.121 ( 0.121)\tLoss 4.6127e+06 (4.6127e+06)\tAcc@1   0.64 (  0.64)\tAcc@5   0.64 (  0.64)\n","Test: [ 0/18]\tTime  0.208 ( 0.208)\tLoss 3.8027e+06 (3.8027e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.084 ( 0.098)\tLoss 6.9345e+06 (6.5356e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [45][0/4]\tTime  0.216 ( 0.216)\tData  0.121 ( 0.121)\tLoss 5.2755e+06 (5.2755e+06)\tAcc@1   0.60 (  0.60)\tAcc@5   0.60 (  0.60)\n","Test: [ 0/18]\tTime  0.200 ( 0.200)\tLoss 1.5498e+06 (1.5498e+06)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [10/18]\tTime  0.084 ( 0.099)\tLoss 3.5118e+06 (3.2743e+06)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n"," * Acc@1 0.4764 Acc@5 0.4764 Precision 0.4954 Recall 0.4764 F1 Score 0.4610\n","Epoch: [46][0/4]\tTime  0.230 ( 0.230)\tData  0.139 ( 0.139)\tLoss 1.1911e+06 (1.1911e+06)\tAcc@1   0.72 (  0.72)\tAcc@5   0.72 (  0.72)\n","Test: [ 0/18]\tTime  0.212 ( 0.212)\tLoss 1.9879e+06 (1.9879e+06)\tAcc@1   0.56 (  0.56)\tAcc@5   0.56 (  0.56)\n","Test: [10/18]\tTime  0.089 ( 0.097)\tLoss 4.6854e+06 (4.1304e+06)\tAcc@1   0.56 (  0.57)\tAcc@5   0.56 (  0.57)\n"," * Acc@1 0.5716 Acc@5 0.5716 Precision 0.5099 Recall 0.5716 F1 Score 0.5078\n","Epoch: [47][0/4]\tTime  0.220 ( 0.220)\tData  0.127 ( 0.127)\tLoss 3.7061e+06 (3.7061e+06)\tAcc@1   0.64 (  0.64)\tAcc@5   0.64 (  0.64)\n","Test: [ 0/18]\tTime  0.207 ( 0.207)\tLoss 7.5007e+06 (7.5007e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.086 ( 0.098)\tLoss 1.0877e+07 (1.0053e+07)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [48][0/4]\tTime  0.395 ( 0.395)\tData  0.160 ( 0.160)\tLoss 9.1373e+06 (9.1373e+06)\tAcc@1   0.54 (  0.54)\tAcc@5   0.54 (  0.54)\n","Test: [ 0/18]\tTime  0.284 ( 0.284)\tLoss 2.8671e+06 (2.8671e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.096 ( 0.154)\tLoss 5.4996e+06 (5.0476e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [49][0/4]\tTime  0.224 ( 0.224)\tData  0.131 ( 0.131)\tLoss 2.8247e+06 (2.8247e+06)\tAcc@1   0.58 (  0.58)\tAcc@5   0.58 (  0.58)\n","Test: [ 0/18]\tTime  0.207 ( 0.207)\tLoss 2.1796e+06 (2.1796e+06)\tAcc@1   0.51 (  0.51)\tAcc@5   0.51 (  0.51)\n","Test: [10/18]\tTime  0.087 ( 0.097)\tLoss 5.0942e+06 (4.4245e+06)\tAcc@1   0.44 (  0.44)\tAcc@5   0.44 (  0.44)\n"," * Acc@1 0.4484 Acc@5 0.4484 Precision 0.4922 Recall 0.4484 F1 Score 0.4165\n","Epoch: [50][0/4]\tTime  0.226 ( 0.226)\tData  0.123 ( 0.123)\tLoss 1.8513e+06 (1.8513e+06)\tAcc@1   0.75 (  0.75)\tAcc@5   0.75 (  0.75)\n","Test: [ 0/18]\tTime  0.218 ( 0.218)\tLoss 7.0343e+06 (7.0343e+06)\tAcc@1   0.35 (  0.35)\tAcc@5   0.35 (  0.35)\n","Test: [10/18]\tTime  0.084 ( 0.105)\tLoss 1.0707e+07 (9.7687e+06)\tAcc@1   0.36 (  0.36)\tAcc@5   0.36 (  0.36)\n"," * Acc@1 0.3720 Acc@5 0.3720 Precision 0.4611 Recall 0.3720 F1 Score 0.2123\n","Epoch: [51][0/4]\tTime  0.219 ( 0.219)\tData  0.123 ( 0.123)\tLoss 5.1667e+06 (5.1667e+06)\tAcc@1   0.51 (  0.51)\tAcc@5   0.51 (  0.51)\n","Test: [ 0/18]\tTime  0.204 ( 0.204)\tLoss 4.0728e+06 (4.0728e+06)\tAcc@1   0.40 (  0.40)\tAcc@5   0.40 (  0.40)\n","Test: [10/18]\tTime  0.086 ( 0.098)\tLoss 7.3675e+06 (6.8624e+06)\tAcc@1   0.38 (  0.38)\tAcc@5   0.38 (  0.38)\n"," * Acc@1 0.3862 Acc@5 0.3862 Precision 0.4649 Recall 0.3862 F1 Score 0.2619\n","Epoch: [52][0/4]\tTime  0.209 ( 0.209)\tData  0.119 ( 0.119)\tLoss 4.8773e+06 (4.8773e+06)\tAcc@1   0.58 (  0.58)\tAcc@5   0.58 (  0.58)\n","Test: [ 0/18]\tTime  0.232 ( 0.232)\tLoss 1.7842e+06 (1.7842e+06)\tAcc@1   0.52 (  0.52)\tAcc@5   0.52 (  0.52)\n","Test: [10/18]\tTime  0.088 ( 0.099)\tLoss 4.6276e+06 (4.1342e+06)\tAcc@1   0.50 (  0.49)\tAcc@5   0.50 (  0.49)\n"," * Acc@1 0.4964 Acc@5 0.4964 Precision 0.4955 Recall 0.4964 F1 Score 0.4858\n","Epoch: [53][0/4]\tTime  0.383 ( 0.383)\tData  0.190 ( 0.190)\tLoss 3.8327e+06 (3.8327e+06)\tAcc@1   0.64 (  0.64)\tAcc@5   0.64 (  0.64)\n","Test: [ 0/18]\tTime  0.349 ( 0.349)\tLoss 4.6012e+06 (4.6012e+06)\tAcc@1   0.35 (  0.35)\tAcc@5   0.35 (  0.35)\n","Test: [10/18]\tTime  0.169 ( 0.153)\tLoss 6.5981e+06 (5.7588e+06)\tAcc@1   0.36 (  0.37)\tAcc@5   0.36 (  0.37)\n"," * Acc@1 0.3720 Acc@5 0.3720 Precision 0.4558 Recall 0.3720 F1 Score 0.2173\n","Epoch: [54][0/4]\tTime  0.283 ( 0.283)\tData  0.177 ( 0.177)\tLoss 2.0679e+06 (2.0679e+06)\tAcc@1   0.53 (  0.53)\tAcc@5   0.53 (  0.53)\n","Test: [ 0/18]\tTime  0.213 ( 0.213)\tLoss 3.0052e+06 (3.0052e+06)\tAcc@1   0.37 (  0.37)\tAcc@5   0.37 (  0.37)\n","Test: [10/18]\tTime  0.087 ( 0.097)\tLoss 4.2716e+06 (4.0349e+06)\tAcc@1   0.36 (  0.38)\tAcc@5   0.36 (  0.38)\n"," * Acc@1 0.3813 Acc@5 0.3813 Precision 0.4651 Recall 0.3813 F1 Score 0.2507\n","Epoch: [55][0/4]\tTime  0.213 ( 0.213)\tData  0.122 ( 0.122)\tLoss 1.7601e+06 (1.7601e+06)\tAcc@1   0.66 (  0.66)\tAcc@5   0.66 (  0.66)\n","Test: [ 0/18]\tTime  0.204 ( 0.204)\tLoss 1.7228e+06 (1.7228e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.085 ( 0.098)\tLoss 3.0277e+06 (2.8387e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [56][0/4]\tTime  0.208 ( 0.208)\tData  0.119 ( 0.119)\tLoss 2.9845e+06 (2.9845e+06)\tAcc@1   0.64 (  0.64)\tAcc@5   0.64 (  0.64)\n","Test: [ 0/18]\tTime  0.206 ( 0.206)\tLoss 2.5515e+06 (2.5515e+06)\tAcc@1   0.38 (  0.38)\tAcc@5   0.38 (  0.38)\n","Test: [10/18]\tTime  0.087 ( 0.098)\tLoss 4.1516e+06 (3.8275e+06)\tAcc@1   0.37 (  0.38)\tAcc@5   0.37 (  0.38)\n"," * Acc@1 0.3796 Acc@5 0.3796 Precision 0.4523 Recall 0.3796 F1 Score 0.2455\n","Epoch: [57][0/4]\tTime  0.210 ( 0.210)\tData  0.118 ( 0.118)\tLoss 2.7190e+06 (2.7190e+06)\tAcc@1   0.54 (  0.54)\tAcc@5   0.54 (  0.54)\n","Test: [ 0/18]\tTime  0.217 ( 0.217)\tLoss 2.2520e+06 (2.2520e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.085 ( 0.100)\tLoss 3.9245e+06 (3.4224e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [58][0/4]\tTime  0.229 ( 0.229)\tData  0.126 ( 0.126)\tLoss 3.3916e+06 (3.3916e+06)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.382 ( 0.382)\tLoss 5.3787e+06 (5.3787e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.113 ( 0.152)\tLoss 7.3774e+06 (6.7323e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [59][0/4]\tTime  0.374 ( 0.374)\tData  0.185 ( 0.185)\tLoss 5.1621e+06 (5.1621e+06)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.229 ( 0.229)\tLoss 7.2800e+06 (7.2800e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.103 ( 0.101)\tLoss 1.0401e+07 (9.8901e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [60][0/4]\tTime  0.234 ( 0.234)\tData  0.123 ( 0.123)\tLoss 7.1498e+06 (7.1498e+06)\tAcc@1   0.55 (  0.55)\tAcc@5   0.55 (  0.55)\n","Test: [ 0/18]\tTime  0.203 ( 0.203)\tLoss 5.2570e+06 (5.2570e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [10/18]\tTime  0.087 ( 0.099)\tLoss 8.0576e+06 (7.8566e+06)\tAcc@1   0.59 (  0.58)\tAcc@5   0.59 (  0.58)\n"," * Acc@1 0.5747 Acc@5 0.5747 Precision 0.3302 Recall 0.5747 F1 Score 0.4194\n","Epoch: [61][0/4]\tTime  0.207 ( 0.207)\tData  0.115 ( 0.115)\tLoss 8.6417e+06 (8.6417e+06)\tAcc@1   0.51 (  0.51)\tAcc@5   0.51 (  0.51)\n","Test: [ 0/18]\tTime  0.208 ( 0.208)\tLoss 6.4939e+06 (6.4939e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [10/18]\tTime  0.093 ( 0.100)\tLoss 9.5096e+06 (8.8329e+06)\tAcc@1   0.35 (  0.36)\tAcc@5   0.35 (  0.36)\n"," * Acc@1 0.3667 Acc@5 0.3667 Precision 0.3260 Recall 0.3667 F1 Score 0.1984\n","Epoch: [62][0/4]\tTime  0.217 ( 0.217)\tData  0.119 ( 0.119)\tLoss 4.4435e+06 (4.4435e+06)\tAcc@1   0.34 (  0.34)\tAcc@5   0.34 (  0.34)\n","Test: [ 0/18]\tTime  0.203 ( 0.203)\tLoss 1.9187e+06 (1.9187e+06)\tAcc@1   0.68 (  0.68)\tAcc@5   0.68 (  0.68)\n","Test: [10/18]\tTime  0.084 ( 0.098)\tLoss 4.4784e+06 (4.1530e+06)\tAcc@1   0.52 (  0.54)\tAcc@5   0.52 (  0.54)\n"," * Acc@1 0.5569 Acc@5 0.5569 Precision 0.5875 Recall 0.5569 F1 Score 0.5385\n","Epoch: [63][0/4]\tTime  0.224 ( 0.224)\tData  0.127 ( 0.127)\tLoss 3.3751e+06 (3.3751e+06)\tAcc@1   0.66 (  0.66)\tAcc@5   0.66 (  0.66)\n","Test: [ 0/18]\tTime  0.313 ( 0.313)\tLoss 1.5975e+06 (1.5975e+06)\tAcc@1   0.69 (  0.69)\tAcc@5   0.69 (  0.69)\n","Test: [10/18]\tTime  0.157 ( 0.165)\tLoss 3.8219e+06 (3.7009e+06)\tAcc@1   0.59 (  0.63)\tAcc@5   0.59 (  0.63)\n"," * Acc@1 0.6280 Acc@5 0.6280 Precision 0.5969 Recall 0.6280 F1 Score 0.6112\n","Epoch: [64][0/4]\tTime  0.388 ( 0.388)\tData  0.211 ( 0.211)\tLoss 3.4654e+06 (3.4654e+06)\tAcc@1   0.63 (  0.63)\tAcc@5   0.63 (  0.63)\n","Test: [ 0/18]\tTime  0.206 ( 0.206)\tLoss 4.1735e+06 (4.1735e+06)\tAcc@1   0.37 (  0.37)\tAcc@5   0.37 (  0.37)\n","Test: [10/18]\tTime  0.085 ( 0.098)\tLoss 6.3645e+06 (5.8116e+06)\tAcc@1   0.36 (  0.37)\tAcc@5   0.36 (  0.37)\n"," * Acc@1 0.3760 Acc@5 0.3760 Precision 0.4606 Recall 0.3760 F1 Score 0.2351\n","Epoch: [65][0/4]\tTime  0.214 ( 0.214)\tData  0.123 ( 0.123)\tLoss 2.9106e+06 (2.9106e+06)\tAcc@1   0.69 (  0.69)\tAcc@5   0.69 (  0.69)\n","Test: [ 0/18]\tTime  0.226 ( 0.226)\tLoss 1.5290e+06 (1.5290e+06)\tAcc@1   0.64 (  0.64)\tAcc@5   0.64 (  0.64)\n","Test: [10/18]\tTime  0.097 ( 0.103)\tLoss 3.5493e+06 (3.3110e+06)\tAcc@1   0.62 (  0.61)\tAcc@5   0.62 (  0.61)\n"," * Acc@1 0.6084 Acc@5 0.6084 Precision 0.5611 Recall 0.6084 F1 Score 0.5649\n","Epoch: [66][0/4]\tTime  0.242 ( 0.242)\tData  0.146 ( 0.146)\tLoss 2.8041e+06 (2.8041e+06)\tAcc@1   0.62 (  0.62)\tAcc@5   0.62 (  0.62)\n","Test: [ 0/18]\tTime  0.216 ( 0.216)\tLoss 3.1193e+06 (3.1193e+06)\tAcc@1   0.35 (  0.35)\tAcc@5   0.35 (  0.35)\n","Test: [10/18]\tTime  0.086 ( 0.099)\tLoss 5.2744e+06 (4.7461e+06)\tAcc@1   0.35 (  0.37)\tAcc@5   0.35 (  0.37)\n"," * Acc@1 0.3782 Acc@5 0.3782 Precision 0.4416 Recall 0.3782 F1 Score 0.2480\n","Epoch: [67][0/4]\tTime  0.218 ( 0.218)\tData  0.122 ( 0.122)\tLoss 1.9774e+06 (1.9774e+06)\tAcc@1   0.58 (  0.58)\tAcc@5   0.58 (  0.58)\n","Test: [ 0/18]\tTime  0.234 ( 0.234)\tLoss 1.4647e+06 (1.4647e+06)\tAcc@1   0.49 (  0.49)\tAcc@5   0.49 (  0.49)\n","Test: [10/18]\tTime  0.088 ( 0.101)\tLoss 3.3663e+06 (2.9677e+06)\tAcc@1   0.51 (  0.50)\tAcc@5   0.51 (  0.50)\n"," * Acc@1 0.5040 Acc@5 0.5040 Precision 0.5751 Recall 0.5040 F1 Score 0.4683\n","Epoch: [68][0/4]\tTime  0.213 ( 0.213)\tData  0.117 ( 0.117)\tLoss 2.5402e+06 (2.5402e+06)\tAcc@1   0.68 (  0.68)\tAcc@5   0.68 (  0.68)\n","Test: [ 0/18]\tTime  0.337 ( 0.337)\tLoss 1.8676e+06 (1.8676e+06)\tAcc@1   0.44 (  0.44)\tAcc@5   0.44 (  0.44)\n","Test: [10/18]\tTime  0.156 ( 0.164)\tLoss 3.6268e+06 (3.3217e+06)\tAcc@1   0.41 (  0.41)\tAcc@5   0.41 (  0.41)\n"," * Acc@1 0.4142 Acc@5 0.4142 Precision 0.5008 Recall 0.4142 F1 Score 0.3278\n","Epoch: [69][0/4]\tTime  0.351 ( 0.351)\tData  0.191 ( 0.191)\tLoss 2.2754e+06 (2.2754e+06)\tAcc@1   0.66 (  0.66)\tAcc@5   0.66 (  0.66)\n","Test: [ 0/18]\tTime  0.221 ( 0.221)\tLoss 1.7760e+06 (1.7760e+06)\tAcc@1   0.45 (  0.45)\tAcc@5   0.45 (  0.45)\n","Test: [10/18]\tTime  0.101 ( 0.100)\tLoss 3.5250e+06 (3.0555e+06)\tAcc@1   0.42 (  0.42)\tAcc@5   0.42 (  0.42)\n"," * Acc@1 0.4173 Acc@5 0.4173 Precision 0.5125 Recall 0.4173 F1 Score 0.3329\n","Epoch: [70][0/4]\tTime  0.243 ( 0.243)\tData  0.145 ( 0.145)\tLoss 1.9290e+06 (1.9290e+06)\tAcc@1   0.68 (  0.68)\tAcc@5   0.68 (  0.68)\n","Test: [ 0/18]\tTime  0.209 ( 0.209)\tLoss 1.1215e+06 (1.1215e+06)\tAcc@1   0.52 (  0.52)\tAcc@5   0.52 (  0.52)\n","Test: [10/18]\tTime  0.085 ( 0.102)\tLoss 2.6425e+06 (2.2973e+06)\tAcc@1   0.45 (  0.48)\tAcc@5   0.45 (  0.48)\n"," * Acc@1 0.4849 Acc@5 0.4849 Precision 0.5638 Recall 0.4849 F1 Score 0.4405\n","Epoch: [71][0/4]\tTime  0.226 ( 0.226)\tData  0.119 ( 0.119)\tLoss 2.0935e+06 (2.0935e+06)\tAcc@1   0.63 (  0.63)\tAcc@5   0.63 (  0.63)\n","Test: [ 0/18]\tTime  0.240 ( 0.240)\tLoss 1.8320e+06 (1.8320e+06)\tAcc@1   0.41 (  0.41)\tAcc@5   0.41 (  0.41)\n","Test: [10/18]\tTime  0.084 ( 0.100)\tLoss 3.1678e+06 (2.7284e+06)\tAcc@1   0.38 (  0.39)\tAcc@5   0.38 (  0.39)\n"," * Acc@1 0.4031 Acc@5 0.4031 Precision 0.4928 Recall 0.4031 F1 Score 0.3073\n","Epoch: [72][0/4]\tTime  0.257 ( 0.257)\tData  0.134 ( 0.134)\tLoss 1.8944e+06 (1.8944e+06)\tAcc@1   0.68 (  0.68)\tAcc@5   0.68 (  0.68)\n","Test: [ 0/18]\tTime  0.212 ( 0.212)\tLoss 8.2079e+05 (8.2079e+05)\tAcc@1   0.53 (  0.53)\tAcc@5   0.53 (  0.53)\n","Test: [10/18]\tTime  0.088 ( 0.106)\tLoss 1.9433e+06 (1.6601e+06)\tAcc@1   0.41 (  0.47)\tAcc@5   0.41 (  0.47)\n"," * Acc@1 0.4867 Acc@5 0.4867 Precision 0.5819 Recall 0.4867 F1 Score 0.4538\n","Epoch: [73][0/4]\tTime  0.219 ( 0.219)\tData  0.123 ( 0.123)\tLoss 1.7804e+06 (1.7804e+06)\tAcc@1   0.69 (  0.69)\tAcc@5   0.69 (  0.69)\n","Test: [ 0/18]\tTime  0.337 ( 0.337)\tLoss 1.8634e+06 (1.8634e+06)\tAcc@1   0.38 (  0.38)\tAcc@5   0.38 (  0.38)\n","Test: [10/18]\tTime  0.140 ( 0.155)\tLoss 3.0194e+06 (2.5311e+06)\tAcc@1   0.39 (  0.38)\tAcc@5   0.39 (  0.38)\n"," * Acc@1 0.3876 Acc@5 0.3876 Precision 0.4537 Recall 0.3876 F1 Score 0.2797\n","Epoch: [74][0/4]\tTime  0.365 ( 0.365)\tData  0.232 ( 0.232)\tLoss 1.5552e+06 (1.5552e+06)\tAcc@1   0.64 (  0.64)\tAcc@5   0.64 (  0.64)\n","Test: [ 0/18]\tTime  0.245 ( 0.245)\tLoss 9.0313e+05 (9.0313e+05)\tAcc@1   0.52 (  0.52)\tAcc@5   0.52 (  0.52)\n","Test: [10/18]\tTime  0.085 ( 0.104)\tLoss 2.0694e+06 (1.6835e+06)\tAcc@1   0.51 (  0.48)\tAcc@5   0.51 (  0.48)\n"," * Acc@1 0.4822 Acc@5 0.4822 Precision 0.5580 Recall 0.4822 F1 Score 0.4390\n","Epoch: [75][0/4]\tTime  0.216 ( 0.216)\tData  0.125 ( 0.125)\tLoss 1.0448e+06 (1.0448e+06)\tAcc@1   0.72 (  0.72)\tAcc@5   0.72 (  0.72)\n","Test: [ 0/18]\tTime  0.211 ( 0.211)\tLoss 1.4411e+06 (1.4411e+06)\tAcc@1   0.45 (  0.45)\tAcc@5   0.45 (  0.45)\n","Test: [10/18]\tTime  0.087 ( 0.098)\tLoss 2.4719e+06 (2.0592e+06)\tAcc@1   0.39 (  0.42)\tAcc@5   0.39 (  0.42)\n"," * Acc@1 0.4307 Acc@5 0.4307 Precision 0.5358 Recall 0.4307 F1 Score 0.3515\n","Epoch: [76][0/4]\tTime  0.217 ( 0.217)\tData  0.125 ( 0.125)\tLoss 1.5314e+06 (1.5314e+06)\tAcc@1   0.69 (  0.69)\tAcc@5   0.69 (  0.69)\n","Test: [ 0/18]\tTime  0.214 ( 0.214)\tLoss 8.4682e+05 (8.4682e+05)\tAcc@1   0.51 (  0.51)\tAcc@5   0.51 (  0.51)\n","Test: [10/18]\tTime  0.085 ( 0.099)\tLoss 1.9276e+06 (1.5685e+06)\tAcc@1   0.44 (  0.46)\tAcc@5   0.44 (  0.46)\n"," * Acc@1 0.4587 Acc@5 0.4587 Precision 0.5617 Recall 0.4587 F1 Score 0.4181\n","Epoch: [77][0/4]\tTime  0.224 ( 0.224)\tData  0.124 ( 0.124)\tLoss 4.5785e+05 (4.5785e+05)\tAcc@1   0.68 (  0.68)\tAcc@5   0.68 (  0.68)\n","Test: [ 0/18]\tTime  0.206 ( 0.206)\tLoss 1.4269e+06 (1.4269e+06)\tAcc@1   0.41 (  0.41)\tAcc@5   0.41 (  0.41)\n","Test: [10/18]\tTime  0.086 ( 0.099)\tLoss 2.1961e+06 (1.9649e+06)\tAcc@1   0.39 (  0.40)\tAcc@5   0.39 (  0.40)\n"," * Acc@1 0.4040 Acc@5 0.4040 Precision 0.4887 Recall 0.4040 F1 Score 0.3140\n","Epoch: [78][0/4]\tTime  0.220 ( 0.220)\tData  0.123 ( 0.123)\tLoss 1.1654e+06 (1.1654e+06)\tAcc@1   0.65 (  0.65)\tAcc@5   0.65 (  0.65)\n","Test: [ 0/18]\tTime  0.237 ( 0.237)\tLoss 9.1653e+05 (9.1653e+05)\tAcc@1   0.55 (  0.55)\tAcc@5   0.55 (  0.55)\n","Test: [10/18]\tTime  0.123 ( 0.152)\tLoss 1.8495e+06 (1.5045e+06)\tAcc@1   0.46 (  0.48)\tAcc@5   0.46 (  0.48)\n"," * Acc@1 0.4791 Acc@5 0.4791 Precision 0.5770 Recall 0.4791 F1 Score 0.4251\n","Epoch: [79][0/4]\tTime  0.330 ( 0.330)\tData  0.196 ( 0.196)\tLoss 1.0929e+06 (1.0929e+06)\tAcc@1   0.73 (  0.73)\tAcc@5   0.73 (  0.73)\n","Test: [ 0/18]\tTime  0.297 ( 0.297)\tLoss 1.2257e+06 (1.2257e+06)\tAcc@1   0.44 (  0.44)\tAcc@5   0.44 (  0.44)\n","Test: [10/18]\tTime  0.082 ( 0.123)\tLoss 1.8922e+06 (1.7211e+06)\tAcc@1   0.38 (  0.40)\tAcc@5   0.38 (  0.40)\n"," * Acc@1 0.4111 Acc@5 0.4111 Precision 0.5060 Recall 0.4111 F1 Score 0.3323\n","Epoch: [80][0/4]\tTime  0.228 ( 0.228)\tData  0.130 ( 0.130)\tLoss 5.7541e+05 (5.7541e+05)\tAcc@1   0.64 (  0.64)\tAcc@5   0.64 (  0.64)\n","Test: [ 0/18]\tTime  0.228 ( 0.228)\tLoss 1.1332e+06 (1.1332e+06)\tAcc@1   0.45 (  0.45)\tAcc@5   0.45 (  0.45)\n","Test: [10/18]\tTime  0.084 ( 0.101)\tLoss 1.8258e+06 (1.7140e+06)\tAcc@1   0.41 (  0.42)\tAcc@5   0.41 (  0.42)\n"," * Acc@1 0.4218 Acc@5 0.4218 Precision 0.5304 Recall 0.4218 F1 Score 0.3508\n","Epoch: [81][0/4]\tTime  0.227 ( 0.227)\tData  0.135 ( 0.135)\tLoss 3.4969e+05 (3.4969e+05)\tAcc@1   0.69 (  0.69)\tAcc@5   0.69 (  0.69)\n","Test: [ 0/18]\tTime  0.201 ( 0.201)\tLoss 1.1401e+06 (1.1401e+06)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [10/18]\tTime  0.086 ( 0.100)\tLoss 1.8844e+06 (1.6308e+06)\tAcc@1   0.41 (  0.43)\tAcc@5   0.41 (  0.43)\n"," * Acc@1 0.4356 Acc@5 0.4356 Precision 0.5396 Recall 0.4356 F1 Score 0.3734\n","Epoch: [82][0/4]\tTime  0.217 ( 0.217)\tData  0.126 ( 0.126)\tLoss 4.7034e+05 (4.7034e+05)\tAcc@1   0.74 (  0.74)\tAcc@5   0.74 (  0.74)\n","Test: [ 0/18]\tTime  0.223 ( 0.223)\tLoss 9.6097e+05 (9.6097e+05)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [10/18]\tTime  0.088 ( 0.100)\tLoss 1.7060e+06 (1.4830e+06)\tAcc@1   0.41 (  0.42)\tAcc@5   0.41 (  0.42)\n"," * Acc@1 0.4311 Acc@5 0.4311 Precision 0.5258 Recall 0.4311 F1 Score 0.3696\n","Epoch: [83][0/4]\tTime  0.228 ( 0.228)\tData  0.123 ( 0.123)\tLoss 9.9780e+05 (9.9780e+05)\tAcc@1   0.65 (  0.65)\tAcc@5   0.65 (  0.65)\n","Test: [ 0/18]\tTime  0.212 ( 0.212)\tLoss 8.9857e+05 (8.9857e+05)\tAcc@1   0.47 (  0.47)\tAcc@5   0.47 (  0.47)\n","Test: [10/18]\tTime  0.157 ( 0.123)\tLoss 1.7011e+06 (1.4082e+06)\tAcc@1   0.42 (  0.44)\tAcc@5   0.42 (  0.44)\n"," * Acc@1 0.4449 Acc@5 0.4449 Precision 0.5504 Recall 0.4449 F1 Score 0.3879\n","Epoch: [84][0/4]\tTime  0.265 ( 0.265)\tData  0.176 ( 0.176)\tLoss 5.5814e+05 (5.5814e+05)\tAcc@1   0.60 (  0.60)\tAcc@5   0.60 (  0.60)\n","Test: [ 0/18]\tTime  0.302 ( 0.302)\tLoss 8.8481e+05 (8.8481e+05)\tAcc@1   0.49 (  0.49)\tAcc@5   0.49 (  0.49)\n","Test: [10/18]\tTime  0.085 ( 0.146)\tLoss 1.7544e+06 (1.4697e+06)\tAcc@1   0.41 (  0.44)\tAcc@5   0.41 (  0.44)\n"," * Acc@1 0.4484 Acc@5 0.4484 Precision 0.5500 Recall 0.4484 F1 Score 0.3928\n","Epoch: [85][0/4]\tTime  0.236 ( 0.236)\tData  0.124 ( 0.124)\tLoss 7.1714e+05 (7.1714e+05)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.207 ( 0.207)\tLoss 1.0203e+06 (1.0203e+06)\tAcc@1   0.45 (  0.45)\tAcc@5   0.45 (  0.45)\n","Test: [10/18]\tTime  0.084 ( 0.098)\tLoss 1.7368e+06 (1.5078e+06)\tAcc@1   0.41 (  0.43)\tAcc@5   0.41 (  0.43)\n"," * Acc@1 0.4418 Acc@5 0.4418 Precision 0.5487 Recall 0.4418 F1 Score 0.3802\n","Epoch: [86][0/4]\tTime  0.205 ( 0.205)\tData  0.115 ( 0.115)\tLoss 3.3142e+05 (3.3142e+05)\tAcc@1   0.72 (  0.72)\tAcc@5   0.72 (  0.72)\n","Test: [ 0/18]\tTime  0.207 ( 0.207)\tLoss 1.2630e+06 (1.2630e+06)\tAcc@1   0.44 (  0.44)\tAcc@5   0.44 (  0.44)\n","Test: [10/18]\tTime  0.085 ( 0.099)\tLoss 1.9743e+06 (1.6854e+06)\tAcc@1   0.43 (  0.41)\tAcc@5   0.43 (  0.41)\n"," * Acc@1 0.4164 Acc@5 0.4164 Precision 0.5148 Recall 0.4164 F1 Score 0.3393\n","Epoch: [87][0/4]\tTime  0.214 ( 0.214)\tData  0.122 ( 0.122)\tLoss 4.2724e+05 (4.2724e+05)\tAcc@1   0.65 (  0.65)\tAcc@5   0.65 (  0.65)\n","Test: [ 0/18]\tTime  0.217 ( 0.217)\tLoss 1.1432e+06 (1.1432e+06)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [10/18]\tTime  0.090 ( 0.098)\tLoss 1.9981e+06 (1.7312e+06)\tAcc@1   0.37 (  0.42)\tAcc@5   0.37 (  0.42)\n"," * Acc@1 0.4227 Acc@5 0.4227 Precision 0.5344 Recall 0.4227 F1 Score 0.3436\n","Epoch: [88][0/4]\tTime  0.233 ( 0.233)\tData  0.129 ( 0.129)\tLoss 6.0499e+05 (6.0499e+05)\tAcc@1   0.66 (  0.66)\tAcc@5   0.66 (  0.66)\n","Test: [ 0/18]\tTime  0.204 ( 0.204)\tLoss 1.1485e+06 (1.1485e+06)\tAcc@1   0.45 (  0.45)\tAcc@5   0.45 (  0.45)\n","Test: [10/18]\tTime  0.103 ( 0.099)\tLoss 1.8175e+06 (1.6349e+06)\tAcc@1   0.45 (  0.43)\tAcc@5   0.45 (  0.43)\n"," * Acc@1 0.4329 Acc@5 0.4329 Precision 0.5445 Recall 0.4329 F1 Score 0.3643\n","Epoch: [89][0/4]\tTime  0.328 ( 0.328)\tData  0.192 ( 0.192)\tLoss 4.2156e+05 (4.2156e+05)\tAcc@1   0.68 (  0.68)\tAcc@5   0.68 (  0.68)\n","Test: [ 0/18]\tTime  0.322 ( 0.322)\tLoss 9.3805e+05 (9.3805e+05)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [10/18]\tTime  0.130 ( 0.163)\tLoss 1.7732e+06 (1.5278e+06)\tAcc@1   0.40 (  0.43)\tAcc@5   0.40 (  0.43)\n"," * Acc@1 0.4324 Acc@5 0.4324 Precision 0.5372 Recall 0.4324 F1 Score 0.3676\n","Epoch: [90][0/4]\tTime  0.225 ( 0.225)\tData  0.133 ( 0.133)\tLoss 7.4098e+05 (7.4098e+05)\tAcc@1   0.74 (  0.74)\tAcc@5   0.74 (  0.74)\n","Test: [ 0/18]\tTime  0.209 ( 0.209)\tLoss 8.3033e+05 (8.3033e+05)\tAcc@1   0.50 (  0.50)\tAcc@5   0.50 (  0.50)\n","Test: [10/18]\tTime  0.086 ( 0.097)\tLoss 1.4429e+06 (1.3844e+06)\tAcc@1   0.48 (  0.45)\tAcc@5   0.48 (  0.45)\n"," * Acc@1 0.4569 Acc@5 0.4569 Precision 0.5602 Recall 0.4569 F1 Score 0.4092\n","Epoch: [91][0/4]\tTime  0.226 ( 0.226)\tData  0.129 ( 0.129)\tLoss 4.8102e+05 (4.8102e+05)\tAcc@1   0.68 (  0.68)\tAcc@5   0.68 (  0.68)\n","Test: [ 0/18]\tTime  0.233 ( 0.233)\tLoss 9.3566e+05 (9.3566e+05)\tAcc@1   0.49 (  0.49)\tAcc@5   0.49 (  0.49)\n","Test: [10/18]\tTime  0.086 ( 0.099)\tLoss 1.5651e+06 (1.4133e+06)\tAcc@1   0.48 (  0.45)\tAcc@5   0.48 (  0.45)\n"," * Acc@1 0.4547 Acc@5 0.4547 Precision 0.5599 Recall 0.4547 F1 Score 0.4048\n","Epoch: [92][0/4]\tTime  0.216 ( 0.216)\tData  0.125 ( 0.125)\tLoss 4.6978e+05 (4.6978e+05)\tAcc@1   0.59 (  0.59)\tAcc@5   0.59 (  0.59)\n","Test: [ 0/18]\tTime  0.208 ( 0.208)\tLoss 8.8397e+05 (8.8397e+05)\tAcc@1   0.45 (  0.45)\tAcc@5   0.45 (  0.45)\n","Test: [10/18]\tTime  0.088 ( 0.100)\tLoss 1.5898e+06 (1.4174e+06)\tAcc@1   0.49 (  0.44)\tAcc@5   0.49 (  0.44)\n"," * Acc@1 0.4471 Acc@5 0.4471 Precision 0.5420 Recall 0.4471 F1 Score 0.3969\n","Epoch: [93][0/4]\tTime  0.222 ( 0.222)\tData  0.126 ( 0.126)\tLoss 4.6047e+05 (4.6047e+05)\tAcc@1   0.66 (  0.66)\tAcc@5   0.66 (  0.66)\n","Test: [ 0/18]\tTime  0.204 ( 0.204)\tLoss 9.3873e+05 (9.3873e+05)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [10/18]\tTime  0.086 ( 0.098)\tLoss 1.6883e+06 (1.4996e+06)\tAcc@1   0.36 (  0.41)\tAcc@5   0.36 (  0.41)\n"," * Acc@1 0.4262 Acc@5 0.4262 Precision 0.5210 Recall 0.4262 F1 Score 0.3581\n","Epoch: [94][0/4]\tTime  0.366 ( 0.366)\tData  0.185 ( 0.185)\tLoss 8.5438e+05 (8.5438e+05)\tAcc@1   0.73 (  0.73)\tAcc@5   0.73 (  0.73)\n","Test: [ 0/18]\tTime  0.315 ( 0.315)\tLoss 9.7211e+05 (9.7211e+05)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [10/18]\tTime  0.148 ( 0.161)\tLoss 1.6511e+06 (1.4797e+06)\tAcc@1   0.48 (  0.44)\tAcc@5   0.48 (  0.44)\n"," * Acc@1 0.4369 Acc@5 0.4369 Precision 0.5438 Recall 0.4369 F1 Score 0.3697\n","Epoch: [95][0/4]\tTime  0.216 ( 0.216)\tData  0.126 ( 0.126)\tLoss 7.2785e+05 (7.2785e+05)\tAcc@1   0.73 (  0.73)\tAcc@5   0.73 (  0.73)\n","Test: [ 0/18]\tTime  0.224 ( 0.224)\tLoss 1.0028e+06 (1.0028e+06)\tAcc@1   0.46 (  0.46)\tAcc@5   0.46 (  0.46)\n","Test: [10/18]\tTime  0.089 ( 0.100)\tLoss 1.6494e+06 (1.4902e+06)\tAcc@1   0.42 (  0.43)\tAcc@5   0.42 (  0.43)\n"," * Acc@1 0.4333 Acc@5 0.4333 Precision 0.5318 Recall 0.4333 F1 Score 0.3680\n","Epoch: [96][0/4]\tTime  0.215 ( 0.215)\tData  0.124 ( 0.124)\tLoss 9.9036e+05 (9.9036e+05)\tAcc@1   0.72 (  0.72)\tAcc@5   0.72 (  0.72)\n","Test: [ 0/18]\tTime  0.211 ( 0.211)\tLoss 9.2522e+05 (9.2522e+05)\tAcc@1   0.47 (  0.47)\tAcc@5   0.47 (  0.47)\n","Test: [10/18]\tTime  0.108 ( 0.099)\tLoss 1.6427e+06 (1.4507e+06)\tAcc@1   0.41 (  0.43)\tAcc@5   0.41 (  0.43)\n"," * Acc@1 0.4320 Acc@5 0.4320 Precision 0.5259 Recall 0.4320 F1 Score 0.3626\n","Epoch: [97][0/4]\tTime  0.239 ( 0.239)\tData  0.136 ( 0.136)\tLoss 7.6207e+05 (7.6207e+05)\tAcc@1   0.66 (  0.66)\tAcc@5   0.66 (  0.66)\n","Test: [ 0/18]\tTime  0.202 ( 0.202)\tLoss 1.0566e+06 (1.0566e+06)\tAcc@1   0.45 (  0.45)\tAcc@5   0.45 (  0.45)\n","Test: [10/18]\tTime  0.085 ( 0.098)\tLoss 1.6178e+06 (1.4624e+06)\tAcc@1   0.41 (  0.44)\tAcc@5   0.41 (  0.44)\n"," * Acc@1 0.4431 Acc@5 0.4431 Precision 0.5507 Recall 0.4431 F1 Score 0.3797\n","Epoch: [98][0/4]\tTime  0.210 ( 0.210)\tData  0.117 ( 0.117)\tLoss 4.0012e+05 (4.0012e+05)\tAcc@1   0.70 (  0.70)\tAcc@5   0.70 (  0.70)\n","Test: [ 0/18]\tTime  0.195 ( 0.195)\tLoss 1.0036e+06 (1.0036e+06)\tAcc@1   0.48 (  0.48)\tAcc@5   0.48 (  0.48)\n","Test: [10/18]\tTime  0.084 ( 0.097)\tLoss 1.6112e+06 (1.4301e+06)\tAcc@1   0.41 (  0.44)\tAcc@5   0.41 (  0.44)\n"," * Acc@1 0.4462 Acc@5 0.4462 Precision 0.5530 Recall 0.4462 F1 Score 0.3916\n","Epoch: [99][0/4]\tTime  0.330 ( 0.330)\tData  0.178 ( 0.178)\tLoss 9.7084e+05 (9.7084e+05)\tAcc@1   0.73 (  0.73)\tAcc@5   0.73 (  0.73)\n","Test: [ 0/18]\tTime  0.330 ( 0.330)\tLoss 9.2142e+05 (9.2142e+05)\tAcc@1   0.45 (  0.45)\tAcc@5   0.45 (  0.45)\n","Test: [10/18]\tTime  0.143 ( 0.147)\tLoss 1.7002e+06 (1.4417e+06)\tAcc@1   0.40 (  0.43)\tAcc@5   0.40 (  0.43)\n"," * Acc@1 0.4427 Acc@5 0.4427 Precision 0.5580 Recall 0.4427 F1 Score 0.3849\n"]}],"source":["linclsmain()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPRDqYO8eXHGKqD/fxV/A7H"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}