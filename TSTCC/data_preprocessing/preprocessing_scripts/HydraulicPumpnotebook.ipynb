{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMoh5kj8Nuk75vF4syJRXI/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w34qmJcQTHUZ","executionInfo":{"status":"ok","timestamp":1718355262518,"user_tz":-60,"elapsed":21738,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"}},"outputId":"03e10e08-93ab-45ae-f58f-b846985cae98"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlXoZDIKS-xT","executionInfo":{"status":"ok","timestamp":1718355299030,"user_tz":-60,"elapsed":309,"user":{"displayName":"Samuel Hesketh Fatchen","userId":"06980961491162386089"}},"outputId":"0446953c-8d6e-45d5-9d16-3ff7c2e2d516"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking directories in: /content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/Data/HydraulicPump\n","Processing directory: piston shoes and swashplate wearing\n","Processed 6144 data points from piston shoes and swashplate wearing\n","Processing directory: normal\n","Processed 3072 data points from normal\n","Processing directory: valve plate wearing\n","Processed 4096 data points from valve plate wearing\n","Data sets saved in /content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/TSTCC/data/HydraulicPump\n","Label mapping: {'piston shoes and swashplate wearing': 0, 'normal': 1, 'valve plate wearing': 2}\n","Label 0 (mapped from piston shoes and swashplate wearing): 96 samples\n","Label 1 (mapped from normal): 48 samples\n","Label 2 (mapped from valve plate wearing): 64 samples\n"]}],"source":["import torch\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from collections import Counter\n","\n","# Setup directories\n","parent_directory = '/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/Data/HydraulicPump'\n","output_dir = \"/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/TSTCC/data/HydraulicPump\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Initialize storage for all data and labels\n","all_data = []\n","all_labels = []\n","label_mapping = {}\n","\n","# Directory traversal and file processing\n","print(\"Checking directories in:\", parent_directory)\n","assert os.path.exists(parent_directory), f\"The directory {parent_directory} does not exist.\"\n","\n","for idx, class_dir in enumerate(os.listdir(parent_directory)):\n","    class_path = os.path.join(parent_directory, class_dir)\n","    if os.path.isdir(class_path):\n","        print(f\"Processing directory: {class_dir}\")\n","        label_mapping[class_dir] = idx\n","        class_data = []\n","        for filename in os.listdir(class_path):\n","            file_path = os.path.join(class_path, filename)\n","            with open(file_path, 'r') as file:\n","                # Read and convert all lines to floats, stripping whitespace\n","                data = [float(line.strip()) for line in file]\n","                class_data.extend(data)\n","\n","        if class_data:\n","            all_data.append(class_data)\n","            all_labels.append(idx)\n","            print(f\"Processed {len(class_data)} data points from {class_dir}\")\n","\n","# Handle case where no data is read\n","if not all_data:\n","    raise ValueError(\"No data files were read. Please check the file paths and formats.\")\n","\n","# Decide on sample_length based on your requirements\n","sample_length = 64  # Example value, adjust based on your needs\n","\n","# Process all data into numpy arrays and corresponding labels\n","processed_data = []\n","processed_labels = []\n","for data, label in zip(all_data, all_labels):\n","    num_samples = len(data) // sample_length\n","    data = np.array(data[:num_samples * sample_length]).reshape(-1, sample_length)\n","    labels = np.full((data.shape[0],), label)\n","    processed_data.append(data)\n","    processed_labels.append(labels)\n","\n","# Concatenate all data and labels\n","processed_data = np.concatenate(processed_data, axis=0)\n","processed_labels = np.concatenate(processed_labels, axis=0)\n","\n","# Normalize the data\n","scaler = MinMaxScaler()\n","processed_data = scaler.fit_transform(processed_data)\n","\n","# Split data into training, validation, and test sets\n","X_train, X_temp, y_train, y_temp = train_test_split(processed_data, processed_labels, test_size=0.4, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# Save the datasets as .pt files\n","datasets = {\n","    \"train\": (X_train, y_train),\n","    \"val\": (X_val, y_val),\n","    \"test\": (X_test, y_test)\n","}\n","\n","for set_name, (X, y) in datasets.items():\n","    torch.save({\n","        \"samples\": torch.from_numpy(X).unsqueeze(1),  # Add channel dimension\n","        \"labels\": torch.from_numpy(y)\n","    }, os.path.join(output_dir, f\"{set_name}.pt\"))\n","\n","print(f\"Data sets saved in {output_dir}\")\n","print(f\"Label mapping: {label_mapping}\")\n","\n","label_counts = Counter(processed_labels)\n","for label, count in label_counts.items():\n","    print(f\"Label {label} (mapped from {list(label_mapping.keys())[list(label_mapping.values()).index(label)]}): {count} samples\")\n"]}]}