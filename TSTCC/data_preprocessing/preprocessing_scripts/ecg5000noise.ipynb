{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOptItK9vSOOna9Q8/Hsw5l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vql-C_5gPNTC","outputId":"8577e924-e9e9-455b-cabf-3c8834b66bf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main\n","train_X:  [[0.46201309 0.31787226 0.19937641 ... 0.67115996 0.79776384 0.69238772]\n"," [0.33452154 0.15554603 0.12300494 ... 0.75703392 0.82137986 0.50407044]\n"," [0.40337698 0.35031341 0.18440888 ... 0.69725872 0.79520464 0.62131641]\n"," ...\n"," [0.47369449 0.5836108  0.478422   ... 0.84555805 0.85938308 0.83846256]\n"," [0.51367925 0.55785455 0.3893776  ... 0.66144769 0.68664723 0.6606424 ]\n"," [0.33029106 0.3351179  0.29677796 ... 0.64022805 0.68554661 0.65589166]]\n","noisy:  [[ 0.07083134 -1.12987648  1.68360517 ...  0.58945918 -0.42829108\n","  -1.71283307]\n"," [-0.31102898 -0.24329125  0.19730628 ...  1.63029618 -0.03817048\n","   0.93737558]\n"," [ 0.98151723  1.2101919   2.55789059 ...  0.41000714 -0.47659601\n","   1.26043785]\n"," ...\n"," [-0.28634979 -3.23013819 -0.92579727 ...  0.55453176  0.8615485\n","   1.74547503]\n"," [ 2.88082517  0.79899763 -0.0335869  ...  2.52533307  0.59774745\n","   1.86817682]\n"," [ 0.06423687 -0.69498335 -3.10269381 ...  4.01223553 -0.34051672\n","  -0.83935884]]\n","Label 1: 2627 occurrences\n","Label 2: 1590 occurrences\n","Label 3: 86 occurrences\n","Label 4: 175 occurrences\n","Label 5: 22 occurrences\n","finished\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","import torch\n","from sklearn.preprocessing import MinMaxScaler\n","from collections import Counter\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main\n","path = '/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main'\n","os.chdir(path)\n","\n","train_data_path = \"/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/Data/ECG5000/ECG5000_TRAIN.tsv\"\n","train_data = np.loadtxt(train_data_path)\n","\n","# Extract features (x) and labels (y)\n","train_y = train_data[:, 0].astype(int) - 1\n","train_X = train_data[:, 1:]\n","\n","test_data_path = \"/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/Data/ECG5000/ECG5000_TEST.tsv\"\n","test_data = np.loadtxt(test_data_path)\n","\n","# Extract features (x) and labels (y)\n","test_y = test_data[:, 0].astype(int) - 1\n","test_X = test_data[:, 1:]\n","\n","scaler = MinMaxScaler()\n","train_X = scaler.fit_transform(train_X)\n","print(\"train_X: \", train_X)\n","\n","scaler = MinMaxScaler()\n","test_X = scaler.fit_transform(test_X)\n","\n","# Add Gaussian noise to the data\n","mean = 0\n","std = 1.5  # You can adjust the standard deviation based on the level of noise you want\n","\n","# Add noise to training data\n","noise = np.random.normal(mean, std, train_X.shape)\n","train_X_noisy = train_X + noise\n","print(\"noisy: \", train_X_noisy)\n","\n","# Add noise to test data\n","noise = np.random.normal(mean, std, test_X.shape)\n","test_X_noisy = test_X + noise\n","\n","output_dir = \"/content/drive/MyDrive/FYP_Nur_Time_Series_Representation_using_CL-main/TSTCC/data/ECG5000noise\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","X_train = train_X_noisy\n","y_train = train_y\n","\n","X_val, X_test, y_val, y_test = train_test_split(test_X_noisy, test_y, test_size=0.5, random_state=42)\n","\n","# Count occurrences of each value in array of non-negative ints\n","label_counts = np.bincount(test_y)\n","\n","# Print the counts\n","for label, count in enumerate(label_counts):\n","    print(f\"Label {label + 1}: {count} occurrences\")\n","\n","dat_dict = dict()\n","dat_dict[\"samples\"] = torch.from_numpy(X_train).unsqueeze(1)\n","dat_dict[\"labels\"] = torch.from_numpy(y_train)\n","torch.save(dat_dict, os.path.join(output_dir, \"train.pt\"))\n","\n","dat_dict = dict()\n","dat_dict[\"samples\"] = torch.from_numpy(X_val).unsqueeze(1)\n","dat_dict[\"labels\"] = torch.from_numpy(y_val)\n","torch.save(dat_dict, os.path.join(output_dir, \"val.pt\"))\n","\n","dat_dict = dict()\n","dat_dict[\"samples\"] = torch.from_numpy(X_test).unsqueeze(1)\n","dat_dict[\"labels\"] = torch.from_numpy(y_test)\n","torch.save(dat_dict, os.path.join(output_dir, \"test.pt\"))\n","print(\"finished\")"]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"95wr_2yW76TQ"},"execution_count":null,"outputs":[]}]}